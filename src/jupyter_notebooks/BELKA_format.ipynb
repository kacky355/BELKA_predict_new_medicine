{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWvf8lNdYIfJxBT7ZLyoGN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tM3YuVAXDo7V"},"outputs":[],"source":["from google.colab import userdata, drive\n","import os\n","drive.mount('/content/drive')\n","os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n","os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')"]},{"cell_type":"code","source":["!pip install kaggle\n","!kaggle datasets download -d kacky355/belka-train-valid-tfrecords-1d-preprocessed\n","!unzip belka-train-valid-tfrecords-1d-preprocessed.zip"],"metadata":{"id":"74M-5gmsDq_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install rdkit\n","!pip install lightning\n","!pip install polars\n","\n","!pip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-cuda120\n","!pip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-tf-plugin-cuda120\n","\n","!pip install git+https://github.com/kacky355/my_libraries.git"],"metadata":{"id":"0u-MObD7Dtir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import os\n","import glob\n","\n","import matplotlib.pyplot as plt\n","import gc\n","from tqdm.notebook import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","\n","from nvidia.dali import pipeline_def, Pipeline\n","import nvidia.dali.fn as fn\n","import nvidia.dali.types as types\n","import nvidia.dali.tfrecord as tfrec\n","from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy\n","\n","import math\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchmetrics import AveragePrecision\n","import lightning as L\n","from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor,TQDMProgressBar\n","\n","from sklearn.metrics import average_precision_score as APS\n","\n","import tensorflow as tf\n","\n","from logger.mylogger import get_my_logger"],"metadata":{"id":"h6ziHyrwDw-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile config.py\n","import os\n","import glob\n","\n","class CFG:\n","    DEBUG = True\n","    MODEL_NAME = 'defo'\n","\n","    EPOCHS = 8\n","    BATCH_SIZE = 4096\n","    NBR_FOLDS = 15\n","    NUM_TRAINS = 91_854_569\n","    NUM_VALIDS = 6_561_041\n","    STEPS_PER_EPOCH_TRAIN = (NUM_TRAINS -1) //BATCH_SIZE +1\n","    STEPS_PER_EPOCH_VALID = (NUM_VALIDS -1) //BATCH_SIZE +1\n","\n","\n","    SELECTED_FOLDS = [0]\n","\n","    BASE_DIR = '/content/drive/MyDrive/BELKA_model/kaggle/working'\n","    DATA_SOURCE = '/content'\n","    TRAINS = glob.glob(os.path.join(DATA_SOURCE, 'train/*'))\n","    TRAINS.sort()\n","    TRAIN_IDX = glob.glob(os.path.join(DATA_SOURCE, 'tf_idx', 'train_*.idx'))\n","    TRAIN_IDX.sort()\n","    VALIDS = glob.glob(os.path.join(DATA_SOURCE, 'valid/*'))\n","    VALIDS.sort()\n","    VARID_IDX = glob.glob(os.path.join(DATA_SOURCE, 'tf_idx', 'valid_*.idx'))\n","    VARID_IDX.sort()\n","\n","    SEED = 2024\n","\n","\n","    FEATURES = [f'enc{i}' for i in range(142)]\n","    TARGETS = ['bind1', 'bind2', 'bind3']\n","    COLUMNS = FEATURES + TARGETS\n","\n","    NUM_CLASSES = 3\n","    SEQ_LENGTH = 142\n","\n","\n","    MODEL_PARAM = {\n","        'batch': BATCH_SIZE,\n","        'input_dim': 142,\n","        'hidden_dim': 64,\n","        'input_dim_embedding': 37,\n","        'dropout': 0.1,\n","        'num_heads': 1 if DEBUG else 4,\n","        'num_layers': 1 if DEBUG else 2,\n","        'out_dim': 3,\n","    }\n","\n","\n","    if DEBUG:\n","        EPOCHS = 3\n","        TRAINS = TRAINS[:4]\n","        TRAIN_IDX = TRAIN_IDX[:4]\n"],"metadata":{"id":"GERU3y5VD0R6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile modules.py\n","\n","from config import CFG\n","\n","\n"],"metadata":{"id":"-VbU-7IhExoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile models.py\n","from config import CFG\n","\n","import math\n","\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import lightning as L\n","\n","from sklearn.metrics import average_precision_score as APS\n","\n","\n","\n","class LightningDefaultModel(L.LightningModule):\n","    def __init__(self, batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim):\n","        super().__init__()\n","        self.save_hyperparameters()\n","#         self.average_precision = MulticlassAveragePrecision(num_classes= 3, thresholds= 0.5)\n","        self.val_preds = []\n","        self.val_y = []\n","\n","\n","        self.embedding = nn.Embedding(num_embeddings= input_dim_embedding, embedding_dim= hidden_dim, padding_idx= 0)\n","\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(hidden_dim),\n","            nn.Linear(hidden_dim,out_dim)\n","        )\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","\n","\n","\n","        x = self.mlp_head(x[:,0,:])\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = self.process_batch(batch)\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('train_loss', loss)\n","        del x, y, logits\n","        return loss\n","\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = self.process_batch(batch)\n","        logits = self(x)\n","        preds = torch.sigmoid(logits)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('val_loss', loss)\n","        self.val_preds.append(preds)\n","        self.val_y.append(y)\n","        del x, y, logtis, preds\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        preds = torch.cat(self.val_preds, 0)\n","        y_eval = torch.cat(self.val_y, 0)\n","        self.log('validation APS  CV score =', APS(y_eval, preds, average='micro'))\n","        self.val_preds.clear()\n","        self.val_y.clear()\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('test_loss', loss)\n","        return loss\n","\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr= 1e-3)\n","        return optimizer\n","\n","    def process_batch(self, batch):\n","        X, y = batch[0].copy(), batch[1].copy()\n","        return X, y\n","\n","\n","\n","\n","\n","class DemoModel(L.LightningModule):\n","    def __init__(self, input_dim=142, input_dim_embedding=37, hidden_dim=128, num_filters=32, output_dim=3, lr=1e-3, weight_decay=1e-6):\n","        super(MyModel, self).__init__()\n","        self.save_hyperparameters()\n","\n","        self.embedding = nn.Embedding(num_embeddings=self.hparams.input_dim_embedding, embedding_dim=self.hparams.hidden_dim, padding_idx=0)\n","        self.conv1 = nn.Conv1d(in_channels=self.hparams.hidden_dim, out_channels=self.hparams.num_filters, kernel_size=3, stride=1, padding=0)\n","        self.conv2 = nn.Conv1d(in_channels=self.hparams.num_filters, out_channels=self.hparams.num_filters*2, kernel_size=3, stride=1, padding=0)\n","        self.conv3 = nn.Conv1d(in_channels=self.hparams.num_filters*2, out_channels=self.hparams.num_filters*3, kernel_size=3, stride=1, padding=0)\n","        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n","        self.fc1 = nn.Linear(self.hparams.num_filters*3, 1024)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc2 = nn.Linear(1024, 1024)\n","        self.fc3 = nn.Linear(1024, 512)\n","        self.output = nn.Linear(512, self.hparams.output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x).permute(0,2,1)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = self.global_max_pool(x).squeeze(2)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc3(x))\n","        x = self.dropout(x)\n","        x = self.output(x)\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('val_loss', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n","        return optimizer\n","\n","    def process_batch(self, batch):\n","        X, y = batch\n","        X, y = X.clone(), y.clone()\n","        return X, y"],"metadata":{"id":"Ui_1S1APERz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile DALILmodels.py\n","from config import CFG\n","from models import LightningDefaultModel, DemoModel\n","\n","import tensorflow as tf\n","\n","from nvidia.dali import pipeline_def, Pipeline\n","import nvidia.dali.fn as fn\n","import nvidia.dali.types as types\n","import nvidia.dali.tfrecord as tfrec\n","from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy\n","\n","\n","class DALILDefoModel(LightningDefaultModel):\n","    def __init__(self, batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim):\n","        super().__init__(batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim)\n","\n","    def setup(self,stage=None):\n","        device_id = self.local_rank\n","        shard_id = self.global_rank\n","        num_shards = self.trainer.world_size\n","\n","        train_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.TRAINS,\n","            idxs=CFG.TRAIN_IDX,\n","            seed=CFG.SEED + 2 + device_id*2\n","        )\n","        valid_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.VALIDS,\n","            idxs=CFG.VARID_IDX,\n","            seed=CFG.SEED-2\n","        )\n","\n","        class LightningWrapper(DALIGenericIterator):\n","            def __init__(self, *kargs, **kwargs):\n","                super().__init__(*kargs, **kwargs)\n","            def __next__(self):\n","                out = super().__next__()\n","                out = out[0]\n","                return [out[k] for k in self.output_map]\n","\n","\n","        self.train_loader = LightningWrapper(train_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP)\n","        self.valid_loader = LightningWrapper(valid_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.PARTIAL)\n","\n","    def train_dataloader(self):\n","        return self.train_loader\n","\n","    def val_dataloader(self):\n","        return self.valid_loader\n","\n","\n","class DALILDemoModel(DemoModel):\n","    def __init__(self, *kargs, **kwargs):\n","        super().__init__(*kargs, **kwargs)\n","\n","    def setup(self,stage=None):\n","        device_id = self.local_rank\n","        shard_id = self.global_rank\n","        num_shards = self.trainer.world_size\n","\n","        train_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.TRAINS,\n","            idxs=CFG.TRAIN_IDX,\n","            seed=CFG.SEED + 2 + device_id*2\n","        )\n","        valid_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.VALIDS,\n","            idxs=CFG.VARID_IDX,\n","            seed=CFG.SEED-2\n","        )\n","\n","        class LightningWrapper(DALIGenericIterator):\n","            def __init__(self, *kargs, **kwargs):\n","                super().__init__(*kargs, **kwargs)\n","            def __next__(self):\n","                out = super().__next__()\n","                out = out[0]\n","                return [out[k] for k in self.output_map]\n","\n","\n","        self.train_loader = LightningWrapper(train_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP)\n","        self.valid_loader = LightningWrapper(valid_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.PARTIAL)\n","\n","    def train_dataloader(self):\n","        return self.train_loader\n","\n","    def val_dataloader(self):\n","        return self.valid_loader\n","\n","\n","\n","\n","@pipeline_def\n","def belka_pipeline(device, paths, idxs, seed,shard_id=0, num_shards=1, is_train=True):\n","    device_id = Pipeline.current().device_id\n","\n","    inputs = fn.readers.tfrecord(\n","        path = paths,\n","        index_path = idxs,\n","        features={\n","            \"x\": tfrec.FixedLenFeature([CFG.SEQ_LENGTH], tfrec.int64, 0),\n","            \"y\": tfrec.FixedLenFeature([CFG.NUM_CLASSES], tfrec.float32, .0)\n","        },\n","        random_shuffle=is_train,\n","        num_shards=num_shards,\n","        shard_id=shard_id,\n","        initial_fill=CFG.BATCH_SIZE,\n","        seed=seed,\n","        name='Reader'\n","    )\n","    x = inputs['x']\n","    y = inputs['y']\n","    if device=='cuda':\n","        x = x.gpu()\n","        y = y.gpu()\n","    return x,y"],"metadata":{"id":"0i9N8zL9EVAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile main.py\n","from config import CFG\n","from DALILmodels import DALILDefoModel\n","\n","from logger.mylogger import get_my_logger\n","\n","import os\n","import numpy as np\n","import random\n","import time\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import lightning as L\n","from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor,TQDMProgressBar\n","\n","\n","\n","def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","def set_logger(name):\n","    now = time.localtime()\n","    now = time.strftime(\"%Y-%m-%d-%H-%M-%S\", now)\n","    log_name = f'{name}-{now}.log'\n","    logger = get_my_logger(CFG.BASE_DIR, log_name)\n","    return logger\n","\n","def set_trainer(logger):\n","    early_stop_callback = EarlyStopping(\n","        monitor= 'val_loss',\n","        mode= 'min',\n","        patience= 3,\n","        verbose= True\n","    )\n","\n","    checkpoint_callback = ModelCheckpoint(\n","        dirpath= f'{CFG.BASE_DIR}/models/',\n","        filename= f'model-{{val_loss}}',\n","        monitor= 'val_loss',\n","        save_top_k= 1,\n","        verbose= True,\n","    )\n","\n","    progress_bar_callback = TQDMProgressBar(refresh_rate=1)\n","\n","    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n","\n","    callbacks = [\n","        early_stop_callback,\n","        checkpoint_callback,\n","        progress_bar_callback,\n","        lr_monitor,\n","    ]\n","    logger.info('set callbacks')\n","\n","    if \"PL_TRAINER_GPUS\" in os.environ:\n","        os.environ.pop(\"PL_TRAINER_GPUS\")\n","\n","    trainer = L.Trainer(\n","            max_epochs= CFG.EPOCHS,\n","            callbacks= callbacks,\n","            accelerator= 'auto',\n","            enable_progress_bar= True,\n","            devices= 'auto',\n","            # strategy='ddp',\n","        )\n","    logger.info('trainer has made')\n","\n","def calc_validation_APS(model, model_name, logger):\n","    logger.info('validation_APS start calucurate')\n","    valid_loader = model.val_dataloader()\n","    all_preds = []\n","    all_y = []\n","    model.eval()\n","    with torch.no_grad():\n","        for X, y in valid_loader:\n","            oof = model(X)\n","            all_preds.append(oof)\n","            all_y.append(y)\n","    preds = torch.cat(all_preds, 0)\n","    y_eval = torch.cat(all_y, 0)\n","    logger.info('valid_results: CV score =', APS(y_eval, preds, average='micro'))\n","\n","    val_results = pd.DataFrame({'y_eval': y_eval.reshape(-1).to('cpu').detach().numpy(),\n","                                model_name: preds.reshape(-1).to('cpu').detach().numpy()})\n","    val_results.to_csv(os.path.join(CFG.BASE_DIR, f'val_results_{model_name}.csv'))\n","    logger.info(f'val_results write complite!\\nfile_name: val_results_{model_name}.csv')\n","\n","def make_submit(model, logger):\n","    logger.info('load test data')\n","    test_data = pd.read_csv('/kaggle/input/leash-BELKA/test.csv')\n","    test_data = TensorDataset(torch.tensor(test_data))\n","    test_loader= DataLoader(test_data)\n","    logger.info('predict test data')\n","    test_preds=[]\n","    model.eval()\n","    with torch.no_grad():\n","        for (X,) in test_loader:\n","            oof = model(X)\n","            test_preds.append(oof)\n","    preds= torch.cat(test_preds, 0).detach().numpy()\n","\n","    logger.info('writing submission.csv start')\n","    tst = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n","    tst['binds'] = 0\n","    tst.loc[tst['protein_name']=='BRD4', 'binds'] = preds[(tst['protein_name']=='BRD4').values, 0]\n","    tst.loc[tst['protein_name']=='HSA', 'binds'] = preds[(tst['protein_name']=='HSA').values, 1]\n","    tst.loc[tst['protein_name']=='sEH', 'binds'] = preds[(tst['protein_name']=='sEH').values, 2]\n","    tst[['id', 'binds']].to_csv('submission.csv', index = False)\n","    logger.info('writing complete')\n","\n","\n","\n","if __name__ == '__main__':\n","    logger = set_logger(CFG.MODEL_NAME)\n","\n","    set_seeds(seed= CFG.SEED)\n","    logger.info(f'set seed: {CFG.SEED}')\n","\n","    model_module = DALILTfEncoder(**CFG.MODEL_PARAM)\n","    logger.info(f'model has made.\\n model:{model_module}')\n","\n","    trainer = set_trainer(logger)\n","    logger.info('training begin')\n","    trainer.fit(model_module)\n","    logger.info('training finish!')\n","    model_module = model_module.load_from_checkpoint(checkpoint_callback.best_model_path)\n","    calc_validation_APS(model_module, CFG.MODEL_NAME, logger)\n","    make_submit(model_module, logger)"],"metadata":{"id":"ShH46MinEbZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python main.py"],"metadata":{"id":"e-uBH-8KEg8g"},"execution_count":null,"outputs":[]}]}