{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM+sI8bBiY15IEEEYYPBP4Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"tM3YuVAXDo7V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719978951921,"user_tz":-540,"elapsed":29809,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"d3720cee-cba5-40ad-a6d6-1fc427dd5428"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import userdata, drive\n","import os\n","drive.mount('/content/drive')\n","os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n","os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')"]},{"cell_type":"code","source":["!pip install kaggle\n","!kaggle datasets download -d kacky355/belka-train-valid-tfrecords-1d-preprocessed\n","!unzip belka-train-valid-tfrecords-1d-preprocessed.zip"],"metadata":{"id":"74M-5gmsDq_m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719979189691,"user_tz":-540,"elapsed":237773,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"dbc0fb1b-38da-4ea0-8913-5e8d100a46bc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n","Dataset URL: https://www.kaggle.com/datasets/kacky355/belka-train-valid-tfrecords-1d-preprocessed\n","License(s): unknown\n","Downloading belka-train-valid-tfrecords-1d-preprocessed.zip to /content\n"," 99% 1.96G/1.98G [00:27<00:00, 107MB/s]\n","100% 1.98G/1.98G [00:27<00:00, 77.7MB/s]\n","Archive:  belka-train-valid-tfrecords-1d-preprocessed.zip\n","  inflating: logs/main.log           \n","  inflating: tf_idx/train_00.idx     \n","  inflating: tf_idx/train_01.idx     \n","  inflating: tf_idx/train_02.idx     \n","  inflating: tf_idx/train_03.idx     \n","  inflating: tf_idx/train_04.idx     \n","  inflating: tf_idx/train_05.idx     \n","  inflating: tf_idx/train_06.idx     \n","  inflating: tf_idx/train_07.idx     \n","  inflating: tf_idx/train_08.idx     \n","  inflating: tf_idx/train_09.idx     \n","  inflating: tf_idx/train_10.idx     \n","  inflating: tf_idx/train_11.idx     \n","  inflating: tf_idx/train_12.idx     \n","  inflating: tf_idx/train_13.idx     \n","  inflating: tf_idx/train_14.idx     \n","  inflating: tf_idx/train_15.idx     \n","  inflating: tf_idx/train_16.idx     \n","  inflating: tf_idx/train_17.idx     \n","  inflating: tf_idx/train_18.idx     \n","  inflating: tf_idx/train_19.idx     \n","  inflating: tf_idx/train_20.idx     \n","  inflating: tf_idx/train_21.idx     \n","  inflating: tf_idx/train_22.idx     \n","  inflating: tf_idx/train_23.idx     \n","  inflating: tf_idx/valid_0.idx      \n","  inflating: tf_idx/valid_1.idx      \n","  inflating: tf_idx/valid_2.idx      \n","  inflating: tf_idx/valid_3.idx      \n","  inflating: tf_idx/valid_4.idx      \n","  inflating: tf_idx/valid_5.idx      \n","  inflating: tf_idx/valid_6.idx      \n","  inflating: tf_idx/valid_7.idx      \n","  inflating: train/BELKA_train_00.tfrecord  \n","  inflating: train/BELKA_train_01.tfrecord  \n","  inflating: train/BELKA_train_02.tfrecord  \n","  inflating: train/BELKA_train_03.tfrecord  \n","  inflating: train/BELKA_train_04.tfrecord  \n","  inflating: train/BELKA_train_05.tfrecord  \n","  inflating: train/BELKA_train_06.tfrecord  \n","  inflating: train/BELKA_train_07.tfrecord  \n","  inflating: train/BELKA_train_08.tfrecord  \n","  inflating: train/BELKA_train_09.tfrecord  \n","  inflating: train/BELKA_train_10.tfrecord  \n","  inflating: train/BELKA_train_11.tfrecord  \n","  inflating: train/BELKA_train_12.tfrecord  \n","  inflating: train/BELKA_train_13.tfrecord  \n","  inflating: train/BELKA_train_14.tfrecord  \n","  inflating: train/BELKA_train_15.tfrecord  \n","  inflating: train/BELKA_train_16.tfrecord  \n","  inflating: train/BELKA_train_17.tfrecord  \n","  inflating: train/BELKA_train_18.tfrecord  \n","  inflating: train/BELKA_train_19.tfrecord  \n","  inflating: train/BELKA_train_20.tfrecord  \n","  inflating: train/BELKA_train_21.tfrecord  \n","  inflating: train/BELKA_train_22.tfrecord  \n","  inflating: train/BELKA_train_23.tfrecord  \n","  inflating: valid/BELKA_valid.tfrecords.0  \n","  inflating: valid/BELKA_valid.tfrecords.1  \n","  inflating: valid/BELKA_valid.tfrecords.2  \n","  inflating: valid/BELKA_valid.tfrecords.3  \n","  inflating: valid/BELKA_valid.tfrecords.4  \n","  inflating: valid/BELKA_valid.tfrecords.5  \n","  inflating: valid/BELKA_valid.tfrecords.6  \n","  inflating: valid/BELKA_valid.tfrecords.7  \n"]}]},{"cell_type":"code","source":["!pip install rdkit\n","!pip install lightning\n","!pip install polars\n","\n","!pip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-cuda120\n","!pip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-tf-plugin-cuda120\n","\n","!pip install git+https://github.com/kacky355/my_libraries.git"],"metadata":{"id":"0u-MObD7Dtir","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719979347908,"user_tz":-540,"elapsed":158270,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"5e077c2f-5f98-4e48-8ad8-844b04dcfe69"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rdkit\n","  Downloading rdkit-2024.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n","Installing collected packages: rdkit\n","Successfully installed rdkit-2024.3.1\n","Collecting lightning\n","  Downloading lightning-2.3.1-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n","  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.25.2)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n","Requirement already satisfied: torch<4.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.3.0+cu121)\n","Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.4)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n","Collecting pytorch-lightning (from lightning)\n","  Downloading pytorch_lightning-2.3.1-py3-none-any.whl (812 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (67.7.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.15.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=2.0.0->lightning)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.0.0->lightning) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.0.0->lightning) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning, lightning\n","Successfully installed lightning-2.3.1 lightning-utilities-0.11.3.post0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.3.1 torchmetrics-1.4.0.post0\n","Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.20.2)\n","Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n","Collecting nvidia-dali-cuda120\n","  Downloading https://pypi.nvidia.com/nvidia-dali-cuda120/nvidia_dali_cuda120-1.39.0-15829601-py3-none-manylinux2014_x86_64.whl (301.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (1.6.3)\n","Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (0.6.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (0.1.8)\n","Collecting nvidia-nvimgcodec-cu12>=0.2.0 (from nvidia-dali-cuda120)\n","  Downloading https://pypi.nvidia.com/nvidia-nvimgcodec-cu12/nvidia_nvimgcodec_cu12-0.2.0.7-py3-none-manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda120) (0.43.0)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda120) (1.16.0)\n","Installing collected packages: nvidia-nvimgcodec-cu12, nvidia-dali-cuda120\n","Successfully installed nvidia-dali-cuda120-1.39.0 nvidia-nvimgcodec-cu12-0.2.0.7\n","Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n","Collecting nvidia-dali-tf-plugin-cuda120\n","  Downloading https://pypi.nvidia.com/nvidia-dali-tf-plugin-cuda120/nvidia-dali-tf-plugin-cuda120-1.39.0.tar.gz (455 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nvidia-dali-cuda120==1.39.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-tf-plugin-cuda120) (1.39.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (1.6.3)\n","Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (0.6.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (0.1.8)\n","Requirement already satisfied: nvidia-nvimgcodec-cu12>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (0.2.0.7)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (0.43.0)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (1.16.0)\n","Building wheels for collected packages: nvidia-dali-tf-plugin-cuda120\n","  Building wheel for nvidia-dali-tf-plugin-cuda120 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvidia-dali-tf-plugin-cuda120: filename=nvidia_dali_tf_plugin_cuda120-1.39.0-cp310-cp310-linux_x86_64.whl size=128313 sha256=99591b501ddab88b1d93c884a82e1507c83432bc528e5bd698d7802f47f79a4c\n","  Stored in directory: /root/.cache/pip/wheels/41/d2/6e/deb92fca17a5dff9c8146227978214fc8eedd3bbff0b345695\n","Successfully built nvidia-dali-tf-plugin-cuda120\n","Installing collected packages: nvidia-dali-tf-plugin-cuda120\n","Successfully installed nvidia-dali-tf-plugin-cuda120-1.39.0\n","Collecting git+https://github.com/kacky355/my_libraries.git\n","  Cloning https://github.com/kacky355/my_libraries.git to /tmp/pip-req-build-yf3mng3h\n","  Running command git clone --filter=blob:none --quiet https://github.com/kacky355/my_libraries.git /tmp/pip-req-build-yf3mng3h\n","  Resolved https://github.com/kacky355/my_libraries.git to commit b8ca37ca5f41d88759d4a7db944f0825c319d362\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: my-libraries\n","  Building wheel for my-libraries (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for my-libraries: filename=my_libraries-0.1-py3-none-any.whl size=2050 sha256=4a3f0423080c7deb61f80a7d4ca32489d237a17e36b874da6fb151b94164249c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-esye8cj_/wheels/a3/79/57/062013d523c79ab3b054efe649d169800772268f629498b79a\n","Successfully built my-libraries\n","Installing collected packages: my-libraries\n","Successfully installed my-libraries-0.1\n"]}]},{"cell_type":"code","source":["!pip install causal-conv1d>=1.4.0\n","!pip install mamba-ssm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKsbuLNAIFHQ","executionInfo":{"status":"ok","timestamp":1719979398931,"user_tz":-540,"elapsed":51034,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"7d12ec3c-7613-4e30-c0d8-215a26c7c2ef"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mamba-ssm\n","  Downloading mamba_ssm-2.2.1.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.3.0+cu121)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.1)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n","Collecting einops (from mamba-ssm)\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.5.82)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.25.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n","Building wheels for collected packages: mamba-ssm\n","  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.1-cp310-cp310-linux_x86_64.whl size=323803693 sha256=e1f518a2f4a14a81be070dbbd6b0711b983af6eac62b98265d790e8df42a8078\n","  Stored in directory: /root/.cache/pip/wheels/31/44/37/2bd1f5a2ad0219a3aa5ae653b942d1af4645f43f14c3ec961e\n","Successfully built mamba-ssm\n","Installing collected packages: einops, mamba-ssm\n","Successfully installed einops-0.8.0 mamba-ssm-2.2.1\n"]}]},{"cell_type":"code","source":["import random\n","import os\n","import glob\n","\n","import matplotlib.pyplot as plt\n","import gc\n","from tqdm.notebook import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","\n","from nvidia.dali import pipeline_def, Pipeline\n","import nvidia.dali.fn as fn\n","import nvidia.dali.types as types\n","import nvidia.dali.tfrecord as tfrec\n","from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy\n","\n","import math\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchmetrics import AveragePrecision\n","import lightning as L\n","from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor,TQDMProgressBar\n","\n","from sklearn.metrics import average_precision_score as APS\n","\n","import tensorflow as tf\n","\n","from logger.mylogger import get_my_logger"],"metadata":{"id":"h6ziHyrwDw-I","executionInfo":{"status":"ok","timestamp":1719979426194,"user_tz":-540,"elapsed":27275,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%%writefile config.py\n","import os\n","import glob\n","\n","class CFG:\n","    DEBUG = False\n","    MODEL_NAME = 'mamba'\n","\n","    EPOCHS = 8\n","    BATCH_SIZE = 4096\n","    NBR_FOLDS = 15\n","    NUM_TRAINS = 91_854_569\n","    NUM_VALIDS = 6_561_041\n","    STEPS_PER_EPOCH_TRAIN = (NUM_TRAINS -1) //BATCH_SIZE +1\n","    STEPS_PER_EPOCH_VALID = (NUM_VALIDS -1) //BATCH_SIZE +1\n","\n","\n","    SELECTED_FOLDS = [0]\n","\n","    BASE_DIR = '/content/drive/MyDrive/BELKA_model/kaggle/working'\n","    DATA_SOURCE = '/content'\n","    TRAINS = glob.glob(os.path.join(DATA_SOURCE, 'train/*'))\n","    TRAINS.sort()\n","    TRAIN_IDX = glob.glob(os.path.join(DATA_SOURCE, 'tf_idx', 'train_*.idx'))\n","    TRAIN_IDX.sort()\n","    VALIDS = glob.glob(os.path.join(DATA_SOURCE, 'valid/*'))\n","    VALIDS.sort()\n","    VARID_IDX = glob.glob(os.path.join(DATA_SOURCE, 'tf_idx', 'valid_*.idx'))\n","    VARID_IDX.sort()\n","\n","    SEED = 2024\n","\n","\n","    FEATURES = [f'enc{i}' for i in range(142)]\n","    TARGETS = ['bind1', 'bind2', 'bind3']\n","    COLUMNS = FEATURES + TARGETS\n","\n","    NUM_CLASSES = 3\n","    SEQ_LENGTH = 142\n","\n","\n","    MODEL_PARAM = {\n","        'batch': BATCH_SIZE,\n","        'input_dim': SEQ_LENGTH,\n","        'hidden_dim': 128,\n","        'input_dim_embedding': 37,\n","        'dropout': 0.1,\n","        'num_heads': 4,\n","        'num_layers': 3,\n","        'out_dim': 3,\n","    }\n","\n","\n","    if DEBUG:\n","        EPOCHS = 3\n","        TRAINS = TRAINS[:4]\n","        TRAIN_IDX = TRAIN_IDX[:4]\n"],"metadata":{"id":"GERU3y5VD0R6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719982361797,"user_tz":-540,"elapsed":333,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"7c94dd5c-2ec5-44fb-9788-3c47cf43ba5e"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting config.py\n"]}]},{"cell_type":"code","source":["# %%writefile modules.py\n","\n","# from config import CFG\n","\n","\n","# import torch\n","# from torch import nn, Tensor\n","# import torch.nn.functional as F\n","# import torch.optim as optim\n","\n","# class Mamba(nn.Module):\n","#     def __init__(self, ):\n","#         super().__init__()\n","\n","\n","#     def forward(self, x):\n","\n","#         return x"],"metadata":{"id":"-VbU-7IhExoC","executionInfo":{"status":"ok","timestamp":1719981774186,"user_tz":-540,"elapsed":3,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["%%writefile models.py\n","from config import CFG\n","\n","import math\n","\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import lightning as L\n","\n","from sklearn.metrics import average_precision_score as APS\n","\n","from mamba_ssm import Mamba\n","\n","\n","\n","\n","class LMmamba(L.LightningModule):\n","    def __init__(self, batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim):\n","        super().__init__()\n","        self.save_hyperparameters()\n","#         self.average_precision = MulticlassAveragePrecision(num_classes= 3, thresholds= 0.5)\n","        self.val_preds = []\n","        self.val_y = []\n","\n","\n","        self.embedding = nn.Embedding(num_embeddings=input_dim_embedding, embedding_dim=hidden_dim, padding_idx=0)\n","        self.conv1 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, stride=1, padding=0)\n","        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n","        self.mamba_blocks = nn.ModuleList()\n","        for i in range(num_layers):\n","            self.mamba_blocks.append(Mamba(\n","            d_model=hidden_dim,\n","            d_state=16,\n","            d_conv=3,\n","            expand=2\n","        ))\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(hidden_dim),\n","            nn.Linear(hidden_dim,out_dim),\n","        )\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.permute(0,2,1)\n","        x = self.conv1(x)\n","        x = self.batch_norm(x)\n","        x = x.permute(0,2,1)\n","\n","        for block in self.mamba_blocks:\n","            x=block(x)\n","            x = F.dropout(x, self.hparams.dropout)\n","\n","        x = self.mlp_head(x[:,0,:])\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = self.process_batch(batch)\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('train_loss', loss)\n","        del x, y, logits\n","        return loss\n","\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = self.process_batch(batch)\n","        logits = self(x)\n","        preds = torch.sigmoid(logits)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('val_loss', loss)\n","        self.val_preds.append(preds)\n","        self.val_y.append(y)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        preds = torch.cat(self.val_preds, 0).to('cpu').detach().numpy()\n","        y_eval = torch.cat(self.val_y, 0).to('cpu').detach().numpy()\n","        self.log('validation APS  CV score =', APS(y_eval, preds, average='micro'))\n","        self.val_preds.clear()\n","        self.val_y.clear()\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('test_loss', loss)\n","        return loss\n","\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr= 1e-3)\n","        return optimizer\n","\n","    def process_batch(self, batch):\n","        X, y = batch[0].clone().long(), batch[1].clone()\n","        return X, y\n","\n","\n","\n","\n","\n","class DemoModel(L.LightningModule):\n","    def __init__(self, input_dim=142, input_dim_embedding=37, hidden_dim=128, num_filters=32, output_dim=3, lr=1e-3, weight_decay=1e-6):\n","        super().__init__()\n","        self.save_hyperparameters()\n","\n","        self.embedding = nn.Embedding(num_embeddings=self.hparams.input_dim_embedding, embedding_dim=self.hparams.hidden_dim, padding_idx=0)\n","        self.conv1 = nn.Conv1d(in_channels=self.hparams.hidden_dim, out_channels=self.hparams.num_filters, kernel_size=3, stride=1, padding=0)\n","        self.conv2 = nn.Conv1d(in_channels=self.hparams.num_filters, out_channels=self.hparams.num_filters*2, kernel_size=3, stride=1, padding=0)\n","        self.conv3 = nn.Conv1d(in_channels=self.hparams.num_filters*2, out_channels=self.hparams.num_filters*3, kernel_size=3, stride=1, padding=0)\n","        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n","        self.fc1 = nn.Linear(self.hparams.num_filters*3, 1024)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc2 = nn.Linear(1024, 1024)\n","        self.fc3 = nn.Linear(1024, 512)\n","        self.output = nn.Linear(512, self.hparams.output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x).permute(0,2,1)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = self.global_max_pool(x).squeeze(2)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc3(x))\n","        x = self.dropout(x)\n","        x = self.output(x)\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('val_loss', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n","        return optimizer\n","\n","    def process_batch(self, batch):\n","        X, y = batch\n","        X, y = X.clone(), y.clone()\n","        return X, y"],"metadata":{"id":"Ui_1S1APERz7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719981907010,"user_tz":-540,"elapsed":415,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"4c6525bc-aacd-43ef-e21e-b6edd7882d17"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting models.py\n"]}]},{"cell_type":"code","source":["%%writefile DALILmodels.py\n","from config import CFG\n","from models import LMmamba, DemoModel\n","\n","import tensorflow as tf\n","\n","from nvidia.dali import pipeline_def, Pipeline\n","import nvidia.dali.fn as fn\n","import nvidia.dali.types as types\n","import nvidia.dali.tfrecord as tfrec\n","from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy\n","\n","\n","class DALILmamba(LMmamba):\n","    def __init__(self, batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim):\n","        super().__init__(batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim)\n","\n","    def setup(self,stage=None):\n","        device_id = self.local_rank\n","        shard_id = self.global_rank\n","        num_shards = self.trainer.world_size\n","\n","        train_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.TRAINS,\n","            idxs=CFG.TRAIN_IDX,\n","            seed=CFG.SEED + 2 + device_id*2\n","        )\n","        valid_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.VALIDS,\n","            idxs=CFG.VARID_IDX,\n","            seed=CFG.SEED-2\n","        )\n","\n","        class LightningWrapper(DALIGenericIterator):\n","            def __init__(self, *kargs, **kwargs):\n","                super().__init__(*kargs, **kwargs)\n","            def __next__(self):\n","                out = super().__next__()\n","                out = out[0]\n","                return [out[k] for k in self.output_map]\n","\n","\n","        self.train_loader = LightningWrapper(train_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP)\n","        self.valid_loader = LightningWrapper(valid_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.PARTIAL)\n","\n","    def train_dataloader(self):\n","        return self.train_loader\n","\n","    def val_dataloader(self):\n","        return self.valid_loader\n","\n","\n","class DALILDemoModel(DemoModel):\n","    def __init__(self, *kargs, **kwargs):\n","        super().__init__(*kargs, **kwargs)\n","\n","    def setup(self,stage=None):\n","        device_id = self.local_rank\n","        shard_id = self.global_rank\n","        num_shards = self.trainer.world_size\n","\n","        train_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.TRAINS,\n","            idxs=CFG.TRAIN_IDX,\n","            seed=CFG.SEED + 2 + device_id*2\n","        )\n","        valid_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.VALIDS,\n","            idxs=CFG.VARID_IDX,\n","            seed=CFG.SEED-2\n","        )\n","\n","        class LightningWrapper(DALIGenericIterator):\n","            def __init__(self, *kargs, **kwargs):\n","                super().__init__(*kargs, **kwargs)\n","            def __next__(self):\n","                out = super().__next__()\n","                out = out[0]\n","                return [out[k] for k in self.output_map]\n","\n","\n","        self.train_loader = LightningWrapper(train_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP)\n","        self.valid_loader = LightningWrapper(valid_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.PARTIAL)\n","\n","    def train_dataloader(self):\n","        return self.train_loader\n","\n","    def val_dataloader(self):\n","        return self.valid_loader\n","\n","\n","\n","\n","@pipeline_def\n","def belka_pipeline(device, paths, idxs, seed,shard_id=0, num_shards=1, is_train=True):\n","    device_id = Pipeline.current().device_id\n","\n","    inputs = fn.readers.tfrecord(\n","        path = paths,\n","        index_path = idxs,\n","        features={\n","            \"x\": tfrec.FixedLenFeature([CFG.SEQ_LENGTH], tfrec.int64, 0),\n","            \"y\": tfrec.FixedLenFeature([CFG.NUM_CLASSES], tfrec.float32, .0)\n","        },\n","        random_shuffle=is_train,\n","        num_shards=num_shards,\n","        shard_id=shard_id,\n","        initial_fill=CFG.BATCH_SIZE,\n","        seed=seed,\n","        name='Reader'\n","    )\n","    x = inputs['x']\n","    y = inputs['y']\n","    if device=='cuda':\n","        x = x.gpu()\n","        y = y.gpu()\n","    return x,y"],"metadata":{"id":"0i9N8zL9EVAI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719981908460,"user_tz":-540,"elapsed":2,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"03211e20-171e-4bf2-bac4-ebcf3c85116c"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting DALILmodels.py\n"]}]},{"cell_type":"code","source":["%%writefile main.py\n","from config import CFG\n","from DALILmodels import DALILmamba\n","\n","from logger.mylogger import get_my_logger\n","\n","import os\n","import numpy as np\n","import random\n","import time\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import lightning as L\n","from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor,TQDMProgressBar\n","\n","\n","\n","def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","def set_logger(name):\n","    now = time.localtime()\n","    now = time.strftime(\"%Y-%m-%d-%H-%M-%S\", now)\n","    log_name = f'{name}-{now}.log'\n","    logger = get_my_logger(CFG.BASE_DIR, log_name)\n","    return logger\n","\n","def set_trainer(logger):\n","    early_stop_callback = EarlyStopping(\n","        monitor= 'val_loss',\n","        mode= 'min',\n","        patience= 3,\n","        verbose= True\n","    )\n","\n","    checkpoint_callback = ModelCheckpoint(\n","        dirpath= f'{CFG.BASE_DIR}/models/',\n","        filename= f'model-{{val_loss}}',\n","        monitor= 'val_loss',\n","        save_top_k= 1,\n","        verbose= True,\n","    )\n","\n","    progress_bar_callback = TQDMProgressBar(refresh_rate=1)\n","\n","    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n","\n","    callbacks = [\n","        early_stop_callback,\n","        checkpoint_callback,\n","        progress_bar_callback,\n","        lr_monitor,\n","    ]\n","    logger.info('set callbacks')\n","\n","    if \"PL_TRAINER_GPUS\" in os.environ:\n","        os.environ.pop(\"PL_TRAINER_GPUS\")\n","\n","    trainer = L.Trainer(\n","            max_epochs= CFG.EPOCHS,\n","            callbacks= callbacks,\n","            accelerator= 'auto',\n","            enable_progress_bar= True,\n","            devices= 'auto',\n","            # strategy='ddp',\n","        )\n","    logger.info('trainer has made')\n","    return trainer\n","\n","def calc_validation_APS(model, model_name, logger):\n","    logger.info('validation_APS start calucurate')\n","    valid_loader = model.val_dataloader()\n","    all_preds = []\n","    all_y = []\n","    model.eval()\n","    with torch.no_grad():\n","        for X, y in valid_loader:\n","            oof = model(X)\n","            all_preds.append(oof)\n","            all_y.append(y)\n","    preds = torch.cat(all_preds, 0)\n","    y_eval = torch.cat(all_y, 0)\n","    logger.info('valid_results: CV score =', APS(y_eval, preds, average='micro'))\n","\n","    val_results = pd.DataFrame({'y_eval': y_eval.reshape(-1).to('cpu').detach().numpy(),\n","                                model_name: preds.reshape(-1).to('cpu').detach().numpy()})\n","    val_results.to_csv(os.path.join(CFG.BASE_DIR, f'val_results_{model_name}.csv'))\n","    logger.info(f'val_results write complite!\\nfile_name: val_results_{model_name}.csv')\n","\n","def make_submit(model, logger):\n","    logger.info('load test data')\n","    test_data = pd.read_csv('/kaggle/input/leash-BELKA/test.csv')\n","    test_data = TensorDataset(torch.tensor(test_data))\n","    test_loader= DataLoader(test_data)\n","    logger.info('predict test data')\n","    test_preds=[]\n","    model.eval()\n","    with torch.no_grad():\n","        for (X,) in test_loader:\n","            oof = model(X)\n","            test_preds.append(oof)\n","    preds= torch.cat(test_preds, 0).detach().numpy()\n","\n","    logger.info('writing submission.csv start')\n","    tst = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n","    tst['binds'] = 0\n","    tst.loc[tst['protein_name']=='BRD4', 'binds'] = preds[(tst['protein_name']=='BRD4').values, 0]\n","    tst.loc[tst['protein_name']=='HSA', 'binds'] = preds[(tst['protein_name']=='HSA').values, 1]\n","    tst.loc[tst['protein_name']=='sEH', 'binds'] = preds[(tst['protein_name']=='sEH').values, 2]\n","    tst[['id', 'binds']].to_csv('submission.csv', index = False)\n","    logger.info('writing complete')\n","\n","\n","\n","if __name__ == '__main__':\n","    logger = set_logger(CFG.MODEL_NAME)\n","\n","    set_seeds(seed= CFG.SEED)\n","    logger.info(f'set seed: {CFG.SEED}')\n","\n","    model_module = DALILmamba(**CFG.MODEL_PARAM)\n","    logger.info(f'model has made.\\n model:{model_module}')\n","\n","    trainer = set_trainer(logger)\n","    logger.info('training begin')\n","    trainer.fit(model_module)\n","    logger.info('training finish!')\n","    # model_module = model_module.load_from_checkpoint(checkpoint_callback.best_model_path)\n","    # calc_validation_APS(model_module, CFG.MODEL_NAME, logger)\n","    # make_submit(model_module, logger)"],"metadata":{"id":"ShH46MinEbZJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719981908809,"user_tz":-540,"elapsed":2,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"d2ec93ba-2531-4aa3-ec6f-1666418e3dc4"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting main.py\n"]}]},{"cell_type":"code","source":["!python main.py"],"metadata":{"id":"e-uBH-8KEg8g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719982491802,"user_tz":-540,"elapsed":118989,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"c5d9b485-7887-4175-941c-d0fb0ef3f29c"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-03 04:52:57.105205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-03 04:52:57.105252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-03 04:52:57.106506: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-03 04:52:58.245227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-03 04:52:59,753 mamba-2024-07-03-04-52-59.log:27 get_my_logger [INFO]: logger has made. log_dir:/content/drive/MyDrive/BELKA_model/kaggle/working/logs\n","2024-07-03 04:52:59,755 mamba-2024-07-03-04-52-59.log:123 <module> [INFO]: set seed: 2024\n","2024-07-03 04:52:59,765 mamba-2024-07-03-04-52-59.log:126 <module> [INFO]: model has made.\n"," model:DALILmamba(\n","  (embedding): Embedding(37, 128, padding_idx=0)\n","  (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n","  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (mamba_blocks): ModuleList(\n","    (0-2): 3 x Mamba(\n","      (in_proj): Linear(in_features=128, out_features=512, bias=False)\n","      (conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(2,), groups=256)\n","      (act): SiLU()\n","      (x_proj): Linear(in_features=256, out_features=40, bias=False)\n","      (dt_proj): Linear(in_features=8, out_features=256, bias=True)\n","      (out_proj): Linear(in_features=256, out_features=128, bias=False)\n","    )\n","  )\n","  (mlp_head): Sequential(\n","    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","    (1): Linear(in_features=128, out_features=3, bias=True)\n","  )\n",")\n","2024-07-03 04:52:59,767 mamba-2024-07-03-04-52-59.log:58 set_trainer [INFO]: set callbacks\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","2024-07-03 04:52:59,916 mamba-2024-07-03-04-52-59.log:71 set_trainer [INFO]: trainer has made\n","2024-07-03 04:52:59,916 mamba-2024-07-03-04-52-59.log:129 <module> [INFO]: training begin\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/MyDrive/BELKA_model/kaggle/working/models exists and is not empty.\n","Warning: the argument `seed` shadows a Pipeline constructor argument of the same name.\n","Warning: the argument `seed` shadows a Pipeline constructor argument of the same name.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name         | Type        | Params | Mode \n","-----------------------------------------------------\n","0 | embedding    | Embedding   | 4.7 K  | train\n","1 | conv1        | Conv1d      | 49.3 K | train\n","2 | batch_norm   | BatchNorm1d | 256    | train\n","3 | mamba_blocks | ModuleList  | 348 K  | train\n","4 | mlp_head     | Sequential  | 643    | train\n","-----------------------------------------------------\n","403 K     Trainable params\n","0         Non-trainable params\n","403 K     Total params\n","1.614     Total estimated model params size (MB)\n","Epoch 0:   0% 0/22425 [00:00<?, ?it/s] /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 516.27 GiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","Epoch 0:   0% 40/22425 [01:04<10:01:05,  1.61s/it, v_num=12]/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n","2024-07-03 04:54:47,777 mamba-2024-07-03-04-52-59.log:131 <module> [INFO]: training finish!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UklgoMHkNKvT","executionInfo":{"status":"ok","timestamp":1719979654968,"user_tz":-540,"elapsed":2,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}}},"execution_count":13,"outputs":[]}]}