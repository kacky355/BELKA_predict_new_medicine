{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOk2kY6ccdNp87UE5hSqjcz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnWOacSOoNSV","executionInfo":{"status":"ok","timestamp":1719811593617,"user_tz":-540,"elapsed":23870,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"b8943989-e56e-4e18-867b-83a71a236b7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import userdata, drive\n","import os\n","drive.mount('/content/drive')\n","os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n","os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')"]},{"cell_type":"code","source":["!pip install kaggle\n","!kaggle datasets download -d kacky355/belka-train-valid-tfrecords-1d-preprocessed\n","!unzip belka-train-valid-tfrecords-1d-preprocessed.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3-arSYipSBj","executionInfo":{"status":"ok","timestamp":1719811840328,"user_tz":-540,"elapsed":242142,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"958048a2-f012-4c1e-ce4b-67232bfa9cb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n","Dataset URL: https://www.kaggle.com/datasets/kacky355/belka-train-valid-tfrecords-1d-preprocessed\n","License(s): unknown\n","Downloading belka-train-valid-tfrecords-1d-preprocessed.zip to /content\n"," 99% 1.97G/1.98G [00:24<00:00, 76.1MB/s]\n","100% 1.98G/1.98G [00:24<00:00, 86.1MB/s]\n","Archive:  belka-train-valid-tfrecords-1d-preprocessed.zip\n","  inflating: logs/main.log           \n","  inflating: tf_idx/train_00.idx     \n","  inflating: tf_idx/train_01.idx     \n","  inflating: tf_idx/train_02.idx     \n","  inflating: tf_idx/train_03.idx     \n","  inflating: tf_idx/train_04.idx     \n","  inflating: tf_idx/train_05.idx     \n","  inflating: tf_idx/train_06.idx     \n","  inflating: tf_idx/train_07.idx     \n","  inflating: tf_idx/train_08.idx     \n","  inflating: tf_idx/train_09.idx     \n","  inflating: tf_idx/train_10.idx     \n","  inflating: tf_idx/train_11.idx     \n","  inflating: tf_idx/train_12.idx     \n","  inflating: tf_idx/train_13.idx     \n","  inflating: tf_idx/train_14.idx     \n","  inflating: tf_idx/train_15.idx     \n","  inflating: tf_idx/train_16.idx     \n","  inflating: tf_idx/train_17.idx     \n","  inflating: tf_idx/train_18.idx     \n","  inflating: tf_idx/train_19.idx     \n","  inflating: tf_idx/train_20.idx     \n","  inflating: tf_idx/train_21.idx     \n","  inflating: tf_idx/train_22.idx     \n","  inflating: tf_idx/train_23.idx     \n","  inflating: tf_idx/valid_0.idx      \n","  inflating: tf_idx/valid_1.idx      \n","  inflating: tf_idx/valid_2.idx      \n","  inflating: tf_idx/valid_3.idx      \n","  inflating: tf_idx/valid_4.idx      \n","  inflating: tf_idx/valid_5.idx      \n","  inflating: tf_idx/valid_6.idx      \n","  inflating: tf_idx/valid_7.idx      \n","  inflating: train/BELKA_train_00.tfrecord  \n","  inflating: train/BELKA_train_01.tfrecord  \n","  inflating: train/BELKA_train_02.tfrecord  \n","  inflating: train/BELKA_train_03.tfrecord  \n","  inflating: train/BELKA_train_04.tfrecord  \n","  inflating: train/BELKA_train_05.tfrecord  \n","  inflating: train/BELKA_train_06.tfrecord  \n","  inflating: train/BELKA_train_07.tfrecord  \n","  inflating: train/BELKA_train_08.tfrecord  \n","  inflating: train/BELKA_train_09.tfrecord  \n","  inflating: train/BELKA_train_10.tfrecord  \n","  inflating: train/BELKA_train_11.tfrecord  \n","  inflating: train/BELKA_train_12.tfrecord  \n","  inflating: train/BELKA_train_13.tfrecord  \n","  inflating: train/BELKA_train_14.tfrecord  \n","  inflating: train/BELKA_train_15.tfrecord  \n","  inflating: train/BELKA_train_16.tfrecord  \n","  inflating: train/BELKA_train_17.tfrecord  \n","  inflating: train/BELKA_train_18.tfrecord  \n","  inflating: train/BELKA_train_19.tfrecord  \n","  inflating: train/BELKA_train_20.tfrecord  \n","  inflating: train/BELKA_train_21.tfrecord  \n","  inflating: train/BELKA_train_22.tfrecord  \n","  inflating: train/BELKA_train_23.tfrecord  \n","  inflating: valid/BELKA_valid.tfrecords.0  \n","  inflating: valid/BELKA_valid.tfrecords.1  \n","  inflating: valid/BELKA_valid.tfrecords.2  \n","  inflating: valid/BELKA_valid.tfrecords.3  \n","  inflating: valid/BELKA_valid.tfrecords.4  \n","  inflating: valid/BELKA_valid.tfrecords.5  \n","  inflating: valid/BELKA_valid.tfrecords.6  \n","  inflating: valid/BELKA_valid.tfrecords.7  \n"]}]},{"cell_type":"code","source":["\n","!pip install rdkit\n","!pip install lightning\n","!pip install polars\n","\n","!pip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-cuda120\n","!pip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-tf-plugin-cuda120\n","\n","!pip install git+https://github.com/kacky355/my_libraries.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TO6G97kuQub","executionInfo":{"status":"ok","timestamp":1719811996136,"user_tz":-540,"elapsed":155812,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"680b88e5-5924-4bda-8115-9d8a32eed868"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rdkit\n","  Downloading rdkit-2024.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n","Installing collected packages: rdkit\n","Successfully installed rdkit-2024.3.1\n","Collecting lightning\n","  Downloading lightning-2.3.1-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n","  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.25.2)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n","Requirement already satisfied: torch<4.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.3.0+cu121)\n","Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.4)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n","Collecting pytorch-lightning (from lightning)\n","  Downloading pytorch_lightning-2.3.1-py3-none-any.whl (812 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (67.7.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.15.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=2.0.0->lightning)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.0.0->lightning) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.0.0->lightning) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning, lightning\n","Successfully installed lightning-2.3.1 lightning-utilities-0.11.3.post0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.3.1 torchmetrics-1.4.0.post0\n","Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.20.2)\n","Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n","Collecting nvidia-dali-cuda120\n","  Downloading https://pypi.nvidia.com/nvidia-dali-cuda120/nvidia_dali_cuda120-1.39.0-15829601-py3-none-manylinux2014_x86_64.whl (301.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (1.6.3)\n","Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (0.1.8)\n","Collecting nvidia-nvimgcodec-cu12>=0.2.0 (from nvidia-dali-cuda120)\n","  Downloading https://pypi.nvidia.com/nvidia-nvimgcodec-cu12/nvidia_nvimgcodec_cu12-0.2.0.7-py3-none-manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda120) (0.43.0)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda120) (1.16.0)\n","Installing collected packages: nvidia-nvimgcodec-cu12, nvidia-dali-cuda120\n","Successfully installed nvidia-dali-cuda120-1.39.0 nvidia-nvimgcodec-cu12-0.2.0.7\n","Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n","Collecting nvidia-dali-tf-plugin-cuda120\n","  Downloading https://pypi.nvidia.com/nvidia-dali-tf-plugin-cuda120/nvidia-dali-tf-plugin-cuda120-1.39.0.tar.gz (455 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nvidia-dali-cuda120==1.39.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-tf-plugin-cuda120) (1.39.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (1.6.3)\n","Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (0.1.8)\n","Requirement already satisfied: nvidia-nvimgcodec-cu12>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (0.2.0.7)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (0.43.0)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda120==1.39.0->nvidia-dali-tf-plugin-cuda120) (1.16.0)\n","Building wheels for collected packages: nvidia-dali-tf-plugin-cuda120\n","  Building wheel for nvidia-dali-tf-plugin-cuda120 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvidia-dali-tf-plugin-cuda120: filename=nvidia_dali_tf_plugin_cuda120-1.39.0-cp310-cp310-linux_x86_64.whl size=128313 sha256=82c37f4185a6152be60e1c639cc3fbe5157cf0d7f32363156f72d23503e74974\n","  Stored in directory: /root/.cache/pip/wheels/41/d2/6e/deb92fca17a5dff9c8146227978214fc8eedd3bbff0b345695\n","Successfully built nvidia-dali-tf-plugin-cuda120\n","Installing collected packages: nvidia-dali-tf-plugin-cuda120\n","Successfully installed nvidia-dali-tf-plugin-cuda120-1.39.0\n","Collecting git+https://github.com/kacky355/my_libraries.git\n","  Cloning https://github.com/kacky355/my_libraries.git to /tmp/pip-req-build-sn3l4rvz\n","  Running command git clone --filter=blob:none --quiet https://github.com/kacky355/my_libraries.git /tmp/pip-req-build-sn3l4rvz\n","  Resolved https://github.com/kacky355/my_libraries.git to commit b8ca37ca5f41d88759d4a7db944f0825c319d362\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: my-libraries\n","  Building wheel for my-libraries (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for my-libraries: filename=my_libraries-0.1-py3-none-any.whl size=2050 sha256=f03fb72d82068f799996497aedf0542108b34cd1bee6e3c1495cf5f1c40be84e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-22x8bv5o/wheels/a3/79/57/062013d523c79ab3b054efe649d169800772268f629498b79a\n","Successfully built my-libraries\n","Installing collected packages: my-libraries\n","Successfully installed my-libraries-0.1\n"]}]},{"cell_type":"code","source":["import random\n","import os\n","import glob\n","\n","import matplotlib.pyplot as plt\n","import gc\n","from tqdm.notebook import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","\n","from nvidia.dali import pipeline_def, Pipeline\n","import nvidia.dali.fn as fn\n","import nvidia.dali.types as types\n","import nvidia.dali.tfrecord as tfrec\n","from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy\n","\n","import math\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchmetrics import AveragePrecision\n","import lightning as L\n","from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor,TQDMProgressBar\n","\n","from sklearn.metrics import average_precision_score as APS\n","\n","import tensorflow as tf\n","\n","from logger.mylogger import get_my_logger"],"metadata":{"id":"dQZAd9vqua2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile config.py\n","import os\n","import glob\n","\n","class CFG:\n","    DEBUG = True\n","    EPOCHS = 8\n","    BATCH_SIZE = 8192\n","    NBR_FOLDS = 15\n","    NUM_TRAINS = 91_854_569\n","    NUM_VALIDS = 6_561_041\n","    STEPS_PER_EPOCH_TRAIN = (NUM_TRAINS -1) //BATCH_SIZE +1\n","    STEPS_PER_EPOCH_VALID = (NUM_VALIDS -1) //BATCH_SIZE +1\n","\n","\n","    SELECTED_FOLDS = [0]\n","\n","\n","\n","    # DATA_SOURCE = '/kaggle/input/belka-train-valid-tfrecords-1d-preprocessed'\n","    DATA_SOURCE = '/content'\n","    TRAINS = glob.glob(os.path.join(DATA_SOURCE, 'train/*'))\n","    TRAINS.sort()\n","    TRAIN_IDX = glob.glob(os.path.join(DATA_SOURCE, 'tf_idx', 'train_*.idx'))\n","    TRAIN_IDX.sort()\n","    VALIDS = glob.glob(os.path.join(DATA_SOURCE, 'valid/*'))\n","    VALIDS.sort()\n","    VARID_IDX = glob.glob(os.path.join(DATA_SOURCE, 'tf_idx', 'valid_*.idx'))\n","    VARID_IDX.sort()\n","\n","    SEED = 2024\n","\n","    BASE_DIR = '/content/drive/MyDrive/BELKA_model/kaggle/working'\n","\n","    FEATURES = [f'enc{i}' for i in range(142)]\n","    TARGETS = ['bind1', 'bind2', 'bind3']\n","    COLUMNS = FEATURES + TARGETS\n","\n","    NUM_CLASSES = 3\n","    SEQ_LENGTH = 142\n","\n","\n","    TRAINER_PARAMS={\n","        'accelerator': 'auto',\n","    }\n","\n","    MODEL_PARAM = {\n","        'batch': BATCH_SIZE,\n","        'input_dim': 142,\n","        'hidden_dim': 64,\n","        'input_dim_embedding': 37,\n","        'dropout': 0.1,\n","        'num_heads':  4,\n","        'num_layers':  2,\n","        'out_dim': 3,\n","    }\n","\n","\n","    if DEBUG:\n","        EPOCHS = 3\n","        TRAINS = TRAINS[:4]\n","        TRAIN_IDX = TRAIN_IDX[:4]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nuTuyacqug2A","executionInfo":{"status":"ok","timestamp":1719813368440,"user_tz":-540,"elapsed":326,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"8fe1ee80-1030-468e-eccb-46e3b9a2ee57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting config.py\n"]}]},{"cell_type":"code","source":["%%writefile Transformer.py\n","from config import CFG\n","\n","import math\n","\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import torch.optim as optim\n","from torchmetrics import AveragePrecision\n","import lightning as L\n","\n","from sklearn.metrics import average_precision_score as APS\n","\n","\n","\n","class LMTransformerEncoder(L.LightningModule):\n","    def __init__(self, batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim):\n","        super().__init__()\n","        self.save_hyperparameters()\n","#         self.average_precision = MulticlassAveragePrecision(num_classes= 3, thresholds= 0.5)\n","        self.embedding = nn.Embedding(num_embeddings= input_dim_embedding, embedding_dim= hidden_dim, padding_idx= 0)\n","        self.cls_token = nn.Parameter(torch.zeros(1,1,hidden_dim))\n","        self.positional_encoder = PositionalEncoder(d_model= hidden_dim, dropout_p= dropout, max_len = batch)\n","        self.encoder = Encoder(hidden_dim= hidden_dim, num_heads= num_heads, num_layers= num_layers, dropout= dropout)\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(hidden_dim),\n","            nn.Linear(hidden_dim,out_dim)\n","        )\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        cls_tokens = self.cls_token.repeat_interleave(x.shape[0], dim= 0)\n","        x = torch.cat((cls_tokens, x), dim= 1)\n","        x = self.positional_encoder(x)\n","        x = self.encoder(x)\n","        x = self.mlp_head(x[:,0,:])\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = self.process_batch(batch)\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        preds = torch.sigmoid(logits)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('val_loss', loss)\n","\n","#         self.average_precision.update(preds, y)\n","#         self.log('val_avg_precision', self.average_precision, on_step=False, on_epoch=True)\n","        return loss\n","\n","#     def on_validation_epoch_end(self):\n","#         avg_precision = self.average_precision.compute()\n","#         self.log('avg_precision', avg_precision)\n","#         self.average_precision.reset()\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('test_loss', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr= 1e-3)\n","        return optimizer\n","\n","    def process_batch(self, batch):\n","        return batch\n","\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, hidden_dim, num_heads, num_layers, dropout=0.1):\n","        super().__init__()\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model= hidden_dim,\n","            nhead= num_heads,\n","            dim_feedforward= hidden_dim*4,\n","            dropout=dropout,\n","            activation='gelu',\n","            batch_first= True,\n","            norm_first= True,\n","        )\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n","\n","    def forward(self, src):\n","        output = self.encoder(src)\n","        return output\n","\n","\n","\n","class PositionalEncoder(nn.Module):\n","    def __init__(self, d_model, dropout_p, max_len=5000):\n","        super(PositionalEncoder, self).__init__()\n","\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","        pos_encoding = torch.zeros(max_len, d_model)\n","        positions_list = torch.arange(0, max_len, dtype= torch.float).unsqueeze(1)\n","        division_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0))/d_model)\n","\n","        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n","        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n","\n","        pos_encoding = pos_encoding.unsqueeze(0).transpose(0,1)\n","        self.register_buffer('pos_encoding', pos_encoding)\n","\n","    def forward(self, token_embedding: torch.Tensor) -> torch.Tensor:\n","#         residual conection + pos_encodeing\n","        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0),:token_embedding.size(1), :])\n","\n","\n","\n","\n","class MyModel(L.LightningModule):\n","    def __init__(self, input_dim=142, input_dim_embedding=37, hidden_dim=128, num_filters=32, output_dim=3, lr=1e-3, weight_decay=1e-6):\n","        super(MyModel, self).__init__()\n","        self.save_hyperparameters()\n","\n","        self.embedding = nn.Embedding(num_embeddings=self.hparams.input_dim_embedding, embedding_dim=self.hparams.hidden_dim, padding_idx=0)\n","        self.conv1 = nn.Conv1d(in_channels=self.hparams.hidden_dim, out_channels=self.hparams.num_filters, kernel_size=3, stride=1, padding=0)\n","        self.conv2 = nn.Conv1d(in_channels=self.hparams.num_filters, out_channels=self.hparams.num_filters*2, kernel_size=3, stride=1, padding=0)\n","        self.conv3 = nn.Conv1d(in_channels=self.hparams.num_filters*2, out_channels=self.hparams.num_filters*3, kernel_size=3, stride=1, padding=0)\n","        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n","        self.fc1 = nn.Linear(self.hparams.num_filters*3, 1024)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc2 = nn.Linear(1024, 1024)\n","        self.fc3 = nn.Linear(1024, 512)\n","        self.output = nn.Linear(512, self.hparams.output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x).permute(0,2,1)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = self.global_max_pool(x).squeeze(2)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc3(x))\n","        x = self.dropout(x)\n","        x = self.output(x)\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.binary_cross_entropy_with_logits(logits, y)\n","        self.log('val_loss', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n","        return optimizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mvuuvMGuk02","executionInfo":{"status":"ok","timestamp":1719812203615,"user_tz":-540,"elapsed":4,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"69a538c9-fcec-424d-bfe8-56946838c74a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting Transformer.py\n"]}]},{"cell_type":"code","source":["%%writefile DALITransformer.py\n","from config import CFG\n","from Transformer import LMTransformerEncoder\n","\n","import tensorflow as tf\n","\n","from nvidia.dali import pipeline_def, Pipeline\n","import nvidia.dali.fn as fn\n","import nvidia.dali.types as types\n","import nvidia.dali.tfrecord as tfrec\n","from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy\n","\n","\n","class DALILTfEncoder(LMTransformerEncoder):\n","    def __init__(self, batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim):\n","        super().__init__(batch, input_dim, input_dim_embedding, hidden_dim, num_heads, num_layers, dropout, out_dim)\n","\n","    def setup(self,stage=None):\n","        device_id = self.local_rank\n","        shard_id = self.global_rank\n","        num_shards = self.trainer.world_size\n","\n","        train_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.TRAINS,\n","            idxs=CFG.TRAIN_IDX,\n","            seed=CFG.SEED + 2 + device_id*2\n","        )\n","        valid_pipe = belka_pipeline(\n","            batch_size=CFG.BATCH_SIZE,\n","            num_threads=4,\n","            device_id=device_id,\n","            device='cuda',\n","            shard_id=shard_id,\n","            num_shards=num_shards,\n","            paths=CFG.VALIDS,\n","            idxs=CFG.VARID_IDX,\n","            seed=CFG.SEED-2\n","        )\n","\n","        class LightningWrapper(DALIGenericIterator):\n","            def __init__(self, *kargs, **kwargs):\n","                super().__init__(*kargs, **kwargs)\n","            def __next__(self):\n","                out = super().__next__()\n","                out = out[0]\n","                return [out[k] for k in self.output_map]\n","\n","\n","        self.train_loader = LightningWrapper(train_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP)\n","        self.valid_loader = LightningWrapper(valid_pipe, ['X', 'y'],reader_name='Reader', last_batch_policy=LastBatchPolicy.PARTIAL)\n","\n","    def train_dataloader(self):\n","        return self.train_loader\n","\n","    def val_dataloader(self):\n","        return self.valid_loader\n","\n","\n","@pipeline_def\n","def belka_pipeline(device, paths, idxs, seed,shard_id=0, num_shards=1, is_train=True):\n","    device_id = Pipeline.current().device_id\n","\n","    inputs = fn.readers.tfrecord(\n","        path = paths,\n","        index_path = idxs,\n","        features={\n","            \"x\": tfrec.FixedLenFeature([CFG.SEQ_LENGTH], tfrec.int64, 0),\n","            \"y\": tfrec.FixedLenFeature([CFG.NUM_CLASSES], tfrec.float32, .0)\n","        },\n","        random_shuffle=is_train,\n","        num_shards=num_shards,\n","        shard_id=shard_id,\n","        initial_fill=CFG.BATCH_SIZE,\n","        seed=seed,\n","        name='Reader'\n","    )\n","    x = inputs['x']\n","    y = inputs['y']\n","    if device=='cuda':\n","        x = x.gpu()\n","        y = y.gpu()\n","    return x,y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kkTsb5GWvp7h","executionInfo":{"status":"ok","timestamp":1719812022296,"user_tz":-540,"elapsed":6,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"1c88132b-a25c-4570-9198-38460c064fb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing DALITransformer.py\n"]}]},{"cell_type":"code","source":["%%writefile main.py\n","from config import CFG\n","from DALITransformer import DALILTfEncoder\n","\n","from logger.mylogger import get_my_logger\n","\n","import os\n","import numpy as np\n","import random\n","import time\n","\n","import torch\n","\n","import lightning as L\n","from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor,TQDMProgressBar\n","\n","\n","\n","def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    now = time.localtime()\n","    now = time.strftime(\"%Y-%m-%d-%H-%M-%S\", now)\n","    log_name = f'transformer-{now}.log'\n","    logger = get_my_logger(CFG.BASE_DIR, log_name)\n","\n","    set_seeds(seed= CFG.SEED)\n","    logger.info(f'set seed: {CFG.SEED}')\n","\n","    model_module = DALILTfEncoder(**CFG.MODEL_PARAM)\n","    logger.info(f'model has made.\\n model:{model_module}')\n","\n","    early_stop_callback = EarlyStopping(\n","        monitor= 'val_loss',\n","        mode= 'min',\n","        patience= 3,\n","        verbose= True\n","    )\n","\n","    checkpoint_callback = ModelCheckpoint(\n","        dirpath= f'{CFG.BASE_DIR}/models/',\n","        filename= f'model-{{val_loss}}',\n","        monitor= 'val_loss',\n","        save_top_k= 1,\n","        verbose= True,\n","    )\n","\n","    progress_bar_callback = TQDMProgressBar(refresh_rate=1)\n","\n","    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n","\n","    callbacks = [\n","        early_stop_callback,\n","        checkpoint_callback,\n","        progress_bar_callback,\n","        lr_monitor,\n","    ]\n","    logger.info('set callbacks')\n","\n","    if \"PL_TRAINER_GPUS\" in os.environ:\n","        os.environ.pop(\"PL_TRAINER_GPUS\")\n","\n","    trainer = L.Trainer(\n","            max_epochs= CFG.EPOCHS,\n","            callbacks= callbacks,\n","            accelerator= 'auto',\n","            enable_progress_bar= True,\n","            devices= 'auto',\n","        )\n","    logger.info('trainer has made')\n","\n","    logger.info('training begin')\n","    trainer.fit(model_module)\n","    logger.info('training finish!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-MZuwGLwCEw","executionInfo":{"status":"ok","timestamp":1719812022297,"user_tz":-540,"elapsed":5,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"d4f93c10-fc0b-4691-a0ca-264e655795ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main.py\n"]}]},{"cell_type":"code","source":["!python main.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G9o0sumIwNmx","executionInfo":{"status":"ok","timestamp":1719813669260,"user_tz":-540,"elapsed":287936,"user":{"displayName":"柿崎海渡","userId":"09458928912192547398"}},"outputId":"00c6dfe1-4ee0-406a-94d0-6b6806f7023c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-01 05:56:27.665574: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-01 05:56:27.665639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-01 05:56:27.667468: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-01 05:56:29.623403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-01 05:56:32,555 main_log:27 get_my_logger [INFO]: logger has made. log_dir:/content/drive/MyDrive/BELKA_model/kaggle/working/logs\n","2024-07-01 05:56:32,558 main_log:30 <module> [INFO]: set seed: 2024\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","2024-07-01 05:56:32,576 main_log:33 <module> [INFO]: model has made.\n"," model:DALILTfEncoder(\n","  (embedding): Embedding(37, 64, padding_idx=0)\n","  (positional_encoder): PositionalEncoder(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): Encoder(\n","    (encoder): TransformerEncoder(\n","      (layers): ModuleList(\n","        (0-1): 2 x TransformerEncoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","          )\n","          (linear1): Linear(in_features=64, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear2): Linear(in_features=256, out_features=64, bias=True)\n","          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.1, inplace=False)\n","          (dropout2): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (mlp_head): Sequential(\n","    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","    (1): Linear(in_features=64, out_features=3, bias=True)\n","  )\n",")\n","2024-07-01 05:56:32,578 main_log:60 <module> [INFO]: set callbacks\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","2024-07-01 05:56:32,871 main_log:72 <module> [INFO]: trainer has made\n","2024-07-01 05:56:32,872 main_log:74 <module> [INFO]: training begin\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/MyDrive/BELKA_model/kaggle/working/models exists and is not empty.\n","Warning: the argument `seed` shadows a Pipeline constructor argument of the same name.\n","Warning: the argument `seed` shadows a Pipeline constructor argument of the same name.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type              | Params  | Mode \n","------------------------------------------------------------------\n","0 | embedding          | Embedding         | 2.4 K   | train\n","1 | positional_encoder | PositionalEncoder | 0       | train\n","2 | encoder            | Encoder           | 100.0 K | train\n","3 | mlp_head           | Sequential        | 323     | train\n","  | other params       | n/a               | 64      | n/a  \n","------------------------------------------------------------------\n","102 K     Trainable params\n","0         Non-trainable params\n","102 K     Total params\n","0.411     Total estimated model params size (MB)\n","Epoch 0:  10% 180/1868 [04:09<39:00,  1.39s/it, v_num=2]/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n","2024-07-01 06:01:06,360 main_log:76 <module> [INFO]: training finish!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PZSGJmf6wsd4"},"execution_count":null,"outputs":[]}]}