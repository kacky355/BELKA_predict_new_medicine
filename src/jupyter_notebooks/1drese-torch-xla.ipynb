{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8275617,"sourceType":"datasetVersion","datasetId":4914065}],"dockerImageVersionId":30734,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"4PlryEi-qV1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZOQ8XWT1qvhV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Library import"],"metadata":{"id":"T3mPUq0YqPzI"}},{"cell_type":"code","source":["import os\n","import random\n","import gc\n","import numpy as np\n","import polars as pl\n","from tqdm.notebook import tqdm\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import average_precision_score as APS\n","\n","\n","import torch.multiprocessing as mp\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.distributed.parallel_roader as xpl\n","\n","import torch_xla.distributed.xla_backend # Registers `xla://` init_method\n","import torch_xla.experimental.pjrt_backend # Required for torch.distributed on TPU v2 and v3\n","\n"],"metadata":{"id":"jDatgJivqPzJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Set Config"],"metadata":{"id":"0i6zw8d_qPzK"}},{"cell_type":"code","source":["print(f'torch : {torch.__version__}')\n","print(f'torch_xla: {torch_xla.__version__}')"],"metadata":{"id":"tiW2vZ0BqPzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if xm.get_xla_supported_device():\n","    device = xm.xla_device()\n","else:\n","    raise Exception('should use xla device to multi process')\n","    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n","\n","print(f\"Using device: {device}\")"],"metadata":{"id":"QSeEhRBuqPzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CFG:\n","    DEBUG = True\n","    PREPROCESS = False\n","\n","    SEED = 2024\n","\n","    N_ROWS = None if not DEBUG else 100_000\n","\n","    BATCH_SIZE = 4096\n","    EPOCHS = 20 if not DEBUG else 3\n","    NUM_FOLDS = 30 if not DEBUG else 5\n","    SELECTED_FOLDS = [0]\n","    SAVE_EVERY = 3\n","\n","\n","    DATA_SRC = '/kaggle/input/belka-enc-dataset'\n","    WORK_DIR = '/kaggle/working'"],"metadata":{"id":"iAic9hKkqPzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","set_seeds(seed= CFG.SEED)\n",""],"metadata":{"id":"cz1RSdkzqPzL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"dVIOjRlZqPzL"}},{"cell_type":"code","source":["if CFG.PREPROCESS:\n","    enc = {'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n","           '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25, '=': 26,\n","           '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36}\n","    train_raw = pd.read_parquet('/kaggle/input/leash-BELKA/train.parquet')\n","    smiles = train_raw[train_raw['protein_name']=='BRD4']['molecule_smiles'].values\n","    assert (smiles!=train_raw[train_raw['protein_name']=='HSA']['molecule_smiles'].values).sum() == 0\n","    assert (smiles!=train_raw[train_raw['protein_name']=='sEH']['molecule_smiles'].values).sum() == 0\n","    def encode_smile(smile):\n","        tmp = [enc[i] for i in smile]\n","        tmp = tmp + [0]*(142-len(tmp))\n","        return np.array(tmp).astype(np.uint8)\n","\n","    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n","    smiles_enc = np.stack(smiles_enc)\n","    train = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n","    train['bind1'] = train_raw[train_raw['protein_name']=='BRD4']['binds'].values\n","    train['bind2'] = train_raw[train_raw['protein_name']=='HSA']['binds'].values\n","    train['bind3'] = train_raw[train_raw['protein_name']=='sEH']['binds'].values\n","    train.to_parquet('train_enc.parquet')\n","\n","    test_raw = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n","    smiles = test_raw['molecule_smiles'].values\n","\n","    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n","    smiles_enc = np.stack(smiles_enc)\n","    test = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n","    test.to_parquet('test_enc.parquet')\n","else:\n","    train = pl.read_parquet(\n","        source=f'{CFG.DATA_SRC}/train_enc.parquet',\n","        n_rows=CFG.N_ROWS\n","    )\n","    test = pl.read_parquet(\n","        source=f'{CFG.DATA_SRC}/test_enc.parquet',\n","        n_rows=CFG.N_ROWS\n","    )"],"metadata":{"id":"FNnRtq5zqPzL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make Model"],"metadata":{"id":"vLtle_5SqPzM"}},{"cell_type":"code","source":["class SELayer(nn.Module):\n","    def __init__(self,channel, reduction=16):\n","        super().__init__()\n","\n","        self.avg_pooling = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, channel // reduction, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(channel // reduction, channel, bias=False),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        batch, channel, _ = x.shape\n","\n","        y = self.avg_pooling(x).view(batch, channel)\n","        y = self.fc(y).view(batch,channel, 1)\n","        y = x * y.expand_as(x)\n","\n","        return y\n"],"metadata":{"id":"Q7UezasqqPzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_conv1(in_channels, out_channels):\n","    conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n","                          kernel_size=1, stride=1,padding='same',bias=True)\n","    nn.init.kaiming_normal_(conv.weight)\n","    return conv\n","\n","def make_conv3(in_channels, out_channels, stride=1):\n","    conv3 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n","                          kernel_size=3, stride=1, padding='same', bias=True)\n","    nn.init.kaiming_normal_(conv3.weight)\n","    return conv3\n","\n","\n","\n","class BottleneckReSELayer(nn.Module):\n","\n","    def __init__(self,input_size, output_size):\n","        super().__init__()\n","\n","        self.is_io_same = input_size == output_size\n","\n","        hidden_size = output_size // 4\n","\n","        conv1_1 = make_conv1(input_size, hidden_size)\n","        conv3 = make_conv3(hidden_size,hidden_size)\n","        conv1_2 = make_conv1(hidden_size, output_size)\n","        conv1_3 = make_conv1(input_size, output_size)\n","\n","        self.fc1 = nn.Sequential(\n","            conv1_1,\n","            nn.BatchNorm1d(num_features=hidden_size),\n","            nn.ReLU(inplace=True),\n","            conv3,\n","            nn.BatchNorm1d(num_features=hidden_size),\n","            nn.ReLU(inplace=True),\n","            conv1_2,\n","            nn.BatchNorm1d(num_features=output_size),\n","            SELayer(channel=output_size)\n","        )\n","\n","        self.fc2 = nn.Sequential(\n","            conv1_3,\n","            nn.BatchNorm1d(num_features=output_size)\n","        )\n","\n","    def forward(self, x):\n","        identity = x\n","        x = self.fc1(x)\n","        if not self.is_io_same:\n","            identity = self.fc2(identity)\n","\n","        out = x + identity\n","        out = F.relu(out)\n","\n","        return out\n",""],"metadata":{"id":"DchShEOYqPzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ReSEModel(nn.Module):\n","    def __init__(self, enc_dict_size:int, channels:int, rese_layer_size:list, num_class=3):\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(\n","            num_embeddings=enc_dict_size,\n","            embedding_dim=channels,\n","            padding_idx=0,\n","        )\n","        self.btl_rese_layers, out_dim = self._make_rese_layers(channels, rese_layer_size)\n","\n","        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n","        self.mlp_head = nn.Sequential(\n","            nn.Linear(out_dim, 1024),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.1),\n","            nn.Linear(1024, 1024),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.1),\n","            nn.Linear(1024,512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_class)\n","        )\n","\n","    def forward(self, x):\n","        x = self.embedding(x).permute(0,2,1)\n","        x = self.btl_rese_layers(x)\n","        x = self.global_max_pool(x).squeeze(2)\n","        x = self.mlp_head(x)\n","\n","        return x\n","\n","\n","    def _make_rese_layers(self, channels:int, layer_size:list):\n","        btl_rese_layers = []\n","        dim = channels\n","        for i, num_layers in enumerate(layer_size, 1):\n","            btl_rese_layers.append(BottleneckReSELayer(dim, channels*i))\n","            for j in range(num_layers-1):\n","                btl_rese_layers.append(BottleneckReSELayer(channels*i, channels*i))\n","            dim = channels * i\n","        btl_rese_layers = nn.Sequential(*btl_rese_layers)\n","        return btl_rese_layers, dim\n","\n","model = ReSEModel(36, 128, [2,2,3]).to(device)"],"metadata":{"id":"X2QLNpPzqPzM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make Trainer"],"metadata":{"id":"KPJXfUsHqPzM"}},{"cell_type":"code","source":["def prepare_dataloader(dataset:Dataset, batch_size:int, rank:int, world_size:int, test:bool =False):\n","    if test:\n","        sampler=None\n","    else:\n","        sampler=DistributedSampler(dataset, num_replicas=world_size, rank=rank,shuffle=True)\n","\n","    return DataLoader(\n","        dataset=dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        samplar=sampler,\n","        num_workers=2,\n","        pin_memory=True\n","    )"],"metadata":{"id":"nVplqRIoqPzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","    def __init__(\n","        self,\n","        model: torch.nn.Module,\n","        train_data: DataLoader,\n","        eval_data: DataLoader,\n","        optimizer: torch.optim.Optimizer,\n","        save_every: int,\n","    ):\n","        self.device = xm.xla_device()\n","        self.model = model.to(device)\n","        self.train_data = train_data\n","        self.eval_data = eval_data\n","        self.optimizer = optimizer\n","        self.save_every = save_every\n","        self.model = DDP(self.model, gradient_as_bucket_view=True)\n","        self.train_data = xpl.MpDeviceLoader(train_data, device)\n","\n","\n","    def _run_batch(self,X, y):\n","        loss = None\n","        with torch_xla.step():\n","            X, y = X.to(device), y.to(device)\n","            self.optimizer.zero_grad()\n","            pred = model(X)\n","            loss = F.binary_cross_entropy_with_logits(pred, y)\n","            loss.backward()\n","            self.optimizer.step()\n","        return loss\n","\n","    def _run_epoch(self, epoch_count):\n","        self.model.train()\n","\n","        running_loss = 0\n","        total = 0\n","        batch_size = len(next(iter(self.train_data))[0])\n","\n","        print(f\"[TPU{self.device}] Epoch {epoch_count} | Batchsize: {batch_size} | Steps: {len(self.train_data)}\")\n","        self.train_data.sampler.set_epoch(epoch_count)\n","\n","        if xm.is_master_ordinal():\n","            dataloader = tqdm(self.train_data).set_description(f'Train {epoch_count}')\n","            for X, y in dataloader:\n","                loss = self._run_batch(X, y)\n","                running_loss += loss.item()\n","                total += y.size(0)\n","                dataloader.set_postfix(loss=running_loss/total)\n","        else:\n","            for X, y in self.train_data:\n","                self._run_batch(X, y)\n","\n","\n","    def _run_eval(self, epoch_count:int):\n","        running_loss = 0\n","        total = 0\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            for X, y in tqdm(self.eval_data).set_description(f'Valid {epoch_count}'):\n","                X, y = X.to(device), y.to(device)\n","                pred = model(X)\n","                loss = F.binary_cross_entropy_with_logits(pred, y)\n","                running_loss += loss.item()\n","                total += y.size(0)\n","                dataloader.set_postfix(loss=running_loss/total)\n","        return running_loss/total\n","\n","\n","    def _save_checkpoint(self, epoch_count):\n","        ckp = self.model.module.state_dict()\n","        PATH = 'checkpoint.pt'\n","        torch.save(ckp, PATH)\n","        print(f\"Epoch {epoch_count} | Training checkpoint saved at {PATH}\")\n","\n","\n","    def fit(self, max_epochs):\n","        for epoch in max_epochs:\n","            self._run_epoch(epoch)\n","\n","            if xm.is_master_ordinal():\n","                if epoch % self.save_every == 0 and epoch > 0:\n","                    self._save_checkpoint(epoch)\n","\n","                loss = self._run_eval(epoch)\n","                print(f'eval_loss : {loss}')\n"],"metadata":{"id":"qAicdYvZqPzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_dataloader(train_df:pl.DataFrame, rank:int, world_size:int=None):\n","    FEATURES = [f'enc{i}' for i in range(142)]\n","    TARGETS = ['bind1', 'bind2', 'bind3']\n","\n","    skf = StratifiedKFold(n_splits = CFG.NUM_FOLDS, shuffle = True, random_state = 42)\n","\n","    for fold, (train_idx, valid_idx) in enumerate(skf.split(np.arange(len(train)), train[TARGETS].sum_horizontal())):\n","        if fold not in CFG.SELECTED_FOLDS:\n","            continue\n","        print(f'Fold: {fold}')\n","        X_train = torch.tensor(train[train_idx, FEATURES].to_numpy(), dtype= torch.int)\n","        y_train = torch.tensor(train[train_idx, TARGETS].to_numpy(), dtype= torch.float16)\n","        X_eval = torch.tensor(train[valid_idx, FEATURES].to_numpy(), dtype= torch.int)\n","        y_eval = torch.tensor(train[valid_idx, TARGETS].to_numpy(), dtype= torch.float16)\n","\n","        train_dataset = TensorDataset(X_train, y_train)\n","        valid_dataset = TensorDataset(X_eval, y_eval)\n","\n","        print('set datasets')\n","        del X_train,y_train\n","        gc.collect()\n","\n","        train_loader = prepare_dataloader(train_dataset, CFG.BATCH_SIZE, rank, world_size)\n","        print('set train_loader')\n","        valid_loader = prepare_dataloader(valid_dataset, CFG.BATCH_SIZE, rank, world_size, test=True)\n","        print('set valid_loader')\n","        del train_dataset, valid_dataset\n","        gc.collect()\n","    return train_loader, valid_loader, X_eval, y_eval\n",""],"metadata":{"id":"o7JWMD8HqPzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _mp_fn(rank,world_size):\n","    init_process_group(backend='xla', init_method='xla://')\n","    train_loader, valid_loader, X_eval, y_eval = load_dataloader(train_df, rank, world_size)\n","    model= ReSEModel(37, 64, [1])\n","    optimizer = optimizer = Adam(params=model.parameters(), lr=0.0001)\n","    trainer = Trainer(model, train_loader, valid_loader, optimizer, CFG.SAVE_EVERY, device, rank, world_size)\n","    trainer.fit(CFG.EPOCHS)\n",""],"metadata":{"id":"Aaf1WbuwqPzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xmp.spawn(_mp_fn, args=())"],"metadata":{"id":"0oBs48tRqPzN"},"execution_count":null,"outputs":[]}]}