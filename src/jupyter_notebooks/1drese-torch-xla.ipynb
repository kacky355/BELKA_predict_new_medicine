{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"792609e6-f707-44ff-b1b1-f688ed9b8462","_cell_guid":"7ee986b9-d7ce-4175-870e-8fa4a3ac760e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-06T04:35:07.494742Z","iopub.execute_input":"2024-06-06T04:35:07.495113Z","iopub.status.idle":"2024-06-06T04:35:09.480895Z","shell.execute_reply.started":"2024-06-06T04:35:07.495084Z","shell.execute_reply":"2024-06-06T04:35:09.479970Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/belka-enc-dataset/train_enc.parquet\n/kaggle/input/belka-enc-dataset/test_enc.parquet\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Library import","metadata":{}},{"cell_type":"code","source":"!pip install polars","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:09.482666Z","iopub.execute_input":"2024-06-06T04:35:09.483060Z","iopub.status.idle":"2024-06-06T04:35:16.481628Z","shell.execute_reply.started":"2024-06-06T04:35:09.483029Z","shell.execute_reply":"2024-06-06T04:35:16.480380Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting polars\n  Downloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: polars\nSuccessfully installed polars-0.20.31\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport random\nimport gc\nimport numpy as np\nimport polars as pl\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader\nimport torch.nn.functional as F\nfrom torch.optim import Adam\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import average_precision_score as APS\n\n\nimport multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler \nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom torch.distributed import init_process_group, destroy_process_group\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as xpl\n\nimport torch_xla.distributed.xla_backend # Registers `xla://` init_method\nimport torch_xla.experimental.pjrt_backend # Required for torch.distributed on TPU v2 and v3\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:16.483274Z","iopub.execute_input":"2024-06-06T04:35:16.483630Z","iopub.status.idle":"2024-06-06T04:35:47.834481Z","shell.execute_reply.started":"2024-06-06T04:35:16.483592Z","shell.execute_reply":"2024-06-06T04:35:47.833265Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nWARNING:root:Patching torch.distributed state to support multithreading.\nWARNING:root:torch.distributed support on TPU v2 and v3 is experimental and does not support torchrun.\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ.pop('TPU_PROCESS_ADDRESSES')\nos.environ.pop('CLOUD_TPU_TASK_ID')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:47.837628Z","iopub.execute_input":"2024-06-06T04:35:47.838005Z","iopub.status.idle":"2024-06-06T04:35:47.847379Z","shell.execute_reply.started":"2024-06-06T04:35:47.837967Z","shell.execute_reply":"2024-06-06T04:35:47.846587Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'0'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Set Config","metadata":{}},{"cell_type":"code","source":"print(f'torch : {torch.__version__}')\nprint(f'torch_xla: {torch_xla.__version__}')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:47.848241Z","iopub.execute_input":"2024-06-06T04:35:47.848482Z","iopub.status.idle":"2024-06-06T04:35:47.861769Z","shell.execute_reply.started":"2024-06-06T04:35:47.848450Z","shell.execute_reply":"2024-06-06T04:35:47.861019Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch : 2.3.0+cu121\ntorch_xla: 2.3.0+libtpu\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    DEBUG = True\n    PREPROCESS = False\n    \n    SEED = 2024\n    \n    N_ROWS = None if not DEBUG else 100_000\n    \n    BATCH_SIZE = 4096\n    EPOCHS = 20 if not DEBUG else 3\n    NUM_FOLDS = 30 if not DEBUG else 5\n    SELECTED_FOLDS = [0]\n    SAVE_EVERY = 3\n    \n    \n    DATA_SRC = '/kaggle/input/belka-enc-dataset'\n    WORK_DIR = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:47.862754Z","iopub.execute_input":"2024-06-06T04:35:47.862992Z","iopub.status.idle":"2024-06-06T04:35:47.874581Z","shell.execute_reply.started":"2024-06-06T04:35:47.862969Z","shell.execute_reply":"2024-06-06T04:35:47.873867Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\nset_seeds(seed= CFG.SEED)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:47.875668Z","iopub.execute_input":"2024-06-06T04:35:47.875940Z","iopub.status.idle":"2024-06-06T04:35:47.886159Z","shell.execute_reply.started":"2024-06-06T04:35:47.875913Z","shell.execute_reply":"2024-06-06T04:35:47.885375Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# if CFG.PREPROCESS:\n#     enc = {'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n#            '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25, '=': 26,\n#            '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36}\n#     train_raw = pd.read_parquet('/kaggle/input/leash-BELKA/train.parquet')\n#     smiles = train_raw[train_raw['protein_name']=='BRD4']['molecule_smiles'].values\n#     assert (smiles!=train_raw[train_raw['protein_name']=='HSA']['molecule_smiles'].values).sum() == 0\n#     assert (smiles!=train_raw[train_raw['protein_name']=='sEH']['molecule_smiles'].values).sum() == 0\n#     def encode_smile(smile):\n#         tmp = [enc[i] for i in smile]\n#         tmp = tmp + [0]*(142-len(tmp))\n#         return np.array(tmp).astype(np.uint8)\n\n#     smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n#     smiles_enc = np.stack(smiles_enc)\n#     train = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n#     train['bind1'] = train_raw[train_raw['protein_name']=='BRD4']['binds'].values\n#     train['bind2'] = train_raw[train_raw['protein_name']=='HSA']['binds'].values\n#     train['bind3'] = train_raw[train_raw['protein_name']=='sEH']['binds'].values\n#     train.to_parquet('train_enc.parquet')\n\n#     test_raw = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n#     smiles = test_raw['molecule_smiles'].values\n\n#     smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n#     smiles_enc = np.stack(smiles_enc)\n#     test = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n#     test.to_parquet('test_enc.parquet')\n# else:\n#     train = pl.read_parquet(\n#         source=f'{CFG.DATA_SRC}/train_enc.parquet', \n#         n_rows=CFG.N_ROWS\n#     )\n#     test = pl.read_parquet(\n#         source=f'{CFG.DATA_SRC}/test_enc.parquet', \n#         n_rows=CFG.N_ROWS\n#     )","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:47.887436Z","iopub.execute_input":"2024-06-06T04:35:47.887696Z","iopub.status.idle":"2024-06-06T04:35:48.178163Z","shell.execute_reply.started":"2024-06-06T04:35:47.887671Z","shell.execute_reply":"2024-06-06T04:35:48.177050Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Make Model","metadata":{}},{"cell_type":"code","source":"class SELayer(nn.Module):\n    def __init__(self,channel, reduction=16):\n        super().__init__()\n        \n        self.avg_pooling = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid(),\n        )\n        \n    def forward(self, x):\n        batch, channel, _ = x.shape\n        \n        y = self.avg_pooling(x).view(batch, channel)\n        y = self.fc(y).view(batch,channel, 1)\n        y = x * y.expand_as(x)\n        \n        return y\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:48.179226Z","iopub.execute_input":"2024-06-06T04:35:48.179493Z","iopub.status.idle":"2024-06-06T04:35:48.185372Z","shell.execute_reply.started":"2024-06-06T04:35:48.179467Z","shell.execute_reply":"2024-06-06T04:35:48.184524Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def make_conv1(in_channels, out_channels):\n    conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n                          kernel_size=1, stride=1,padding='same',bias=True)\n    nn.init.kaiming_normal_(conv.weight)\n    return conv\n\ndef make_conv3(in_channels, out_channels, stride=1):\n    conv3 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n                          kernel_size=3, stride=1, padding='same', bias=True)\n    nn.init.kaiming_normal_(conv3.weight)\n    return conv3\n\n\n\nclass BottleneckReSELayer(nn.Module):\n    \n    def __init__(self,input_size, output_size):\n        super().__init__()\n        \n        self.is_io_same = input_size == output_size\n        \n        hidden_size = output_size // 4\n        \n        conv1_1 = make_conv1(input_size, hidden_size)\n        conv3 = make_conv3(hidden_size,hidden_size)\n        conv1_2 = make_conv1(hidden_size, output_size)\n        conv1_3 = make_conv1(input_size, output_size)\n        \n        self.fc1 = nn.Sequential(\n            conv1_1,\n            nn.BatchNorm1d(num_features=hidden_size),\n            nn.ReLU(inplace=True),\n            conv3,\n            nn.BatchNorm1d(num_features=hidden_size),\n            nn.ReLU(inplace=True),\n            conv1_2,\n            nn.BatchNorm1d(num_features=output_size),\n            SELayer(channel=output_size)\n        )\n        \n        self.fc2 = nn.Sequential(\n            conv1_3,\n            nn.BatchNorm1d(num_features=output_size)\n        )\n    \n    def forward(self, x):\n        identity = x\n        x = self.fc1(x)\n        if not self.is_io_same:\n            identity = self.fc2(identity)\n        \n        out = x + identity\n        out = F.relu(out)\n        \n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:48.187979Z","iopub.execute_input":"2024-06-06T04:35:48.188250Z","iopub.status.idle":"2024-06-06T04:35:48.201444Z","shell.execute_reply.started":"2024-06-06T04:35:48.188226Z","shell.execute_reply":"2024-06-06T04:35:48.200704Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ReSEModel(nn.Module):\n    def __init__(self, enc_dict_size:int, channels:int, rese_layer_size:list, num_class=3):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(\n            num_embeddings=enc_dict_size,\n            embedding_dim=channels,\n            padding_idx=0,            \n        )\n        self.btl_rese_layers, out_dim = self._make_rese_layers(channels, rese_layer_size)   \n        \n        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n        self.mlp_head = nn.Sequential(\n            nn.Linear(out_dim, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(1024,512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_class)\n        )\n        \n    def forward(self, x):\n        x = self.embedding(x).permute(0,2,1)\n        x = self.btl_rese_layers(x)\n        x = self.global_max_pool(x).squeeze(2)\n        x = self.mlp_head(x)\n        \n        return x\n        \n    \n    def _make_rese_layers(self, channels:int, layer_size:list):\n        btl_rese_layers = []\n        dim = channels\n        for i, num_layers in enumerate(layer_size, 1):\n            btl_rese_layers.append(BottleneckReSELayer(dim, channels*i))\n            for j in range(num_layers-1):\n                btl_rese_layers.append(BottleneckReSELayer(channels*i, channels*i))\n            dim = channels * i\n        btl_rese_layers = nn.Sequential(*btl_rese_layers)\n        return btl_rese_layers, dim\n\nmodel = ReSEModel(36, 128, [2,2,3])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:48.202361Z","iopub.execute_input":"2024-06-06T04:35:48.202614Z","iopub.status.idle":"2024-06-06T04:35:48.268693Z","shell.execute_reply.started":"2024-06-06T04:35:48.202590Z","shell.execute_reply":"2024-06-06T04:35:48.267859Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Make Trainer","metadata":{}},{"cell_type":"code","source":"def prepare_dataloader(dataset:Dataset, batch_size:int, test:bool =False):\n    if test:\n        sampler=None\n    else:\n        sampler=DistributedSampler(dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(),shuffle=True)\n    \n    return DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=not test,\n        sampler=sampler,\n        num_workers=2,\n        pin_memory=True\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:48.269755Z","iopub.execute_input":"2024-06-06T04:35:48.270012Z","iopub.status.idle":"2024-06-06T04:35:48.274644Z","shell.execute_reply.started":"2024-06-06T04:35:48.269987Z","shell.execute_reply":"2024-06-06T04:35:48.273915Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n    def __init__(\n        self, \n        model: torch.nn.Module,\n        train_data: DataLoader,\n        eval_data: DataLoader,\n        optimizer: torch.optim.Optimizer,\n        save_every: int,\n    ):\n        self.device = xm.xla_device()\n        self.model = model.to(device)\n        self.train_data = train_data\n        self.eval_data = eval_data\n        self.optimizer = optimizer\n        self.save_every = save_every\n        self.model = DDP(self.model, gradient_as_bucket_view=True)\n        self.train_data = xpl.MpDeviceLoader(train_data, device)\n\n    \n    def _run_batch(self,X, y):\n        loss = None\n        with torch_xla.step():\n            X, y = X.to(device), y.to(device)\n            self.optimizer.zero_grad()\n            pred = model(X)\n            loss = F.binary_cross_entropy_with_logits(pred, y)\n            loss.backward()\n            self.optimizer.step()\n            xm.mark_step()\n        return loss\n    \n    def _run_epoch(self, epoch_count):        \n        self.model.train()\n        \n        running_loss = 0\n        total = 0\n        batch_size = len(next(iter(self.train_data))[0])\n        \n        print(f\"[TPU{self.device}] Epoch {epoch_count} | Batchsize: {batch_size} | Steps: {len(self.train_data)}\")\n        self.train_data.sampler.set_epoch(epoch_count)\n        \n        if xm.is_master_ordinal():\n            dataloader = tqdm(self.train_data).set_description(f'Train {epoch_count}')\n            for X, y in dataloader:\n                loss = self._run_batch(X, y)\n                running_loss += loss.item()\n                total += y.size(0)\n                dataloader.set_postfix(loss=running_loss/total)\n        else:\n            for X, y in self.train_data:\n                self._run_batch(X, y)\n    \n    \n    def _run_eval(self, epoch_count:int):\n        running_loss = 0\n        total = 0\n        \n        self.model.eval()\n        with torch.no_grad():\n            for X, y in tqdm(self.eval_data).set_description(f'Valid {epoch_count}'):\n                X, y = X.to(device), y.to(device)\n                pred = model(X)\n                loss = F.binary_cross_entropy_with_logits(pred, y)\n                running_loss += loss.item()\n                total += y.size(0)\n                dataloader.set_postfix(loss=running_loss/total)\n        return running_loss/total \n        \n         \n    def _save_checkpoint(self, epoch_count):\n        ckp = self.model.module.state_dict()\n        PATH = 'checkpoint.pt'\n        torch.save(ckp, PATH)\n        print(f\"Epoch {epoch_count} | Training checkpoint saved at {PATH}\")\n\n    \n    def fit(self, max_epochs):        \n        for epoch in max_epochs:\n            self._run_epoch(epoch)\n            \n            if xm.is_master_ordinal():\n                if epoch % self.save_every == 0 and epoch > 0:\n                    self._save_checkpoint(epoch)\n                \n                loss = self._run_eval(epoch)\n                print(f'eval_loss : {loss}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:48.275776Z","iopub.execute_input":"2024-06-06T04:35:48.276126Z","iopub.status.idle":"2024-06-06T04:35:48.291387Z","shell.execute_reply.started":"2024-06-06T04:35:48.276095Z","shell.execute_reply":"2024-06-06T04:35:48.290664Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def load_dataloader(train_df:pl.DataFrame):\n    FEATURES = [f'enc{i}' for i in range(142)]\n    TARGETS = ['bind1', 'bind2', 'bind3']\n\n    skf = StratifiedKFold(n_splits = CFG.NUM_FOLDS, shuffle = True, random_state = 42)\n\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(np.arange(len(train)), train[TARGETS].sum_horizontal())):\n        if fold not in CFG.SELECTED_FOLDS:\n            continue\n        print(f'Fold: {fold}')\n        X_train = torch.tensor(train[train_idx, FEATURES].to_numpy(), dtype= torch.int)\n        y_train = torch.tensor(train[train_idx, TARGETS].to_numpy(), dtype= torch.float16)\n        X_eval = torch.tensor(train[valid_idx, FEATURES].to_numpy(), dtype= torch.int)\n        y_eval = torch.tensor(train[valid_idx, TARGETS].to_numpy(), dtype= torch.float16)\n\n        train_dataset = TensorDataset(X_train, y_train)\n        valid_dataset = TensorDataset(X_eval, y_eval)\n\n        print('set datasets')\n        del X_train,y_train\n        gc.collect()\n        \n        train_loader = prepare_dataloader(train_dataset, CFG.BATCH_SIZE)\n        print('set train_loader')\n        valid_loader = prepare_dataloader(valid_dataset, CFG.BATCH_SIZE, test=True)\n        print('set valid_loader')\n        del train_dataset, valid_dataset\n        gc.collect()\n    return train_loader, valid_loader, X_eval, y_eval \n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:48.292375Z","iopub.execute_input":"2024-06-06T04:35:48.292609Z","iopub.status.idle":"2024-06-06T04:35:48.305643Z","shell.execute_reply.started":"2024-06-06T04:35:48.292586Z","shell.execute_reply":"2024-06-06T04:35:48.304807Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def _mp_fn(rank,lock):\n    device = xm.xla_device()\n    world_size=xm.xrt_world_size()\n    print(f'world_size: {world_size}')\n    init_process_group(backend='xla', init_method='xla://')\n    train_df = pl.read_parquet(\n        source=f'{CFG.DATA_SRC}/train_enc.parquet', \n        n_rows=CFG.N_ROWS\n    )\n    print('dataset loaded complite')\n    train_loader, valid_loader, X_eval, y_eval = load_dataloader(train_df, rank, world_size)\n    model= ReSEModel(37, 64, [1])\n    optimizer = optimizer = Adam(params=model.parameters(), lr=0.0001)\n    trainer = Trainer(model, train_loader, valid_loader, optimizer, CFG.SAVE_EVERY)\n    trainer.fit(CFG.EPOCHS)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:48.306684Z","iopub.execute_input":"2024-06-06T04:35:48.306982Z","iopub.status.idle":"2024-06-06T04:35:48.319403Z","shell.execute_reply.started":"2024-06-06T04:35:48.306925Z","shell.execute_reply":"2024-06-06T04:35:48.318691Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"xmp.spawn(_mp_fn, args=(), start_method='fork')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T04:35:48.320334Z","iopub.execute_input":"2024-06-06T04:35:48.320601Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717648550.227911     324 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\nI0000 00:00:1717648550.227911     326 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717648550.227919     327 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\nI0000 00:00:1717648550.227915     325 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\nI0000 00:00:1717648550.228062     326 pjrt_api.cc:79] PJRT_Api is set for device type tpu\nI0000 00:00:1717648550.228062     324 pjrt_api.cc:79] PJRT_Api is set for device type tpu\nI0000 00:00:1717648550.228062     327 pjrt_api.cc:79] PJRT_Api is set for device type tpu\nI0000 00:00:1717648550.228062     325 pjrt_api.cc:79] PJRT_Api is set for device type tpu\nI0000 00:00:1717648550.228077     326 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\nI0000 00:00:1717648550.228080     324 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\nI0000 00:00:1717648550.228086     327 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\nI0000 00:00:1717648550.228085     325 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\nE0606 04:35:50.271183101     860 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-06-06T04:35:50.271163169+00:00\"}\nE0606 04:35:50.272054166     861 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-06-06T04:35:50.272033806+00:00\", grpc_status:2}\nE0606 04:35:50.272110009     864 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-06-06T04:35:50.27209396+00:00\", grpc_status:2}\nE0606 04:35:50.272250620     859 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-06-06T04:35:50.272234799+00:00\"}\n","output_type":"stream"},{"name":"stdout","text":"world_size: 8world_size: 8\n\nworld_size: 8world_size: 8\n\nworld_size: 8world_size: 8world_size: 8world_size: 8\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}